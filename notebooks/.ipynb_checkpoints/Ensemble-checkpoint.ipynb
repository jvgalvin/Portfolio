{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbd8188b",
   "metadata": {},
   "source": [
    "# Ensemble\n",
    "\n",
    "Feed the final hidden states from Longformer and the final hidden states from Specter into a fully-connected feedforward network for token-level classification\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "603e4761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import LongformerTokenizerFast, AutoTokenizer, TFLongformerModel, TFAutoModel\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "from sklearn.metrics import auc, plot_precision_recall_curve, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46c86744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in data\n",
    "\n",
    "DEV_MASKS_FILE =    \"../data/processed/jg_dev_masks.json\"\n",
    "TRAIN_MASKS_FILE =  \"../data/processed/jg_train_masks.json\"\n",
    "TEST_MASKS_FILE =   \"../data/processed/jg_test_masks.json\"\n",
    "\n",
    "with open(\"../data/raw/text-anonymization-benchmark/echr_dev.json\") as file:\n",
    "    dev_file = json.load(file)\n",
    "\n",
    "with open(DEV_MASKS_FILE) as file:\n",
    "    dev_masks = json.load(file)\n",
    "    \n",
    "with open(\"../data/raw/text-anonymization-benchmark/echr_train.json\") as file:\n",
    "    train_file = json.load(file)\n",
    "    \n",
    "with open(TRAIN_MASKS_FILE) as file:\n",
    "    train_masks = json.load(file)\n",
    "       \n",
    "with open(\"../data/raw/text-anonymization-benchmark/echr_test.json\") as file:\n",
    "    test_file = json.load(file)\n",
    "\n",
    "with open(TEST_MASKS_FILE) as file:\n",
    "    test_masks = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195c8fc7",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "871c636e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to label data\n",
    "\n",
    "def label_tokens(toks, offs, spans_to_mask):\n",
    "    \"\"\"Args: \n",
    "            toks - list of token id's\n",
    "            offs - list of char offsets for each token\n",
    "       Returns:\n",
    "            label_list - 0 for non_mask, 1 for mask\"\"\"\n",
    "    \n",
    "    label_list = []\n",
    "    mapping_list = []\n",
    "    \n",
    "    # Map token_ids back to string\n",
    "    \n",
    "    for token, pos in zip(toks, offs):\n",
    "        mapping_list.append([token, pos[0], pos[1]])\n",
    "    \n",
    "    # Determine if each token should be masked\n",
    "    spans_to_mask.sort(key=lambda tup: tup[0]) #order spans, ascending\n",
    "    \n",
    "    j=0\n",
    "    \n",
    "    for i in range(len(mapping_list)):\n",
    "        \n",
    "        temp_list = []\n",
    "        stop=False\n",
    "        \n",
    "        while not stop and j < len(spans_to_mask):\n",
    "            \n",
    "            if (mapping_list[i][1] >= spans_to_mask[j][0]) and (mapping_list[i][2] <= spans_to_mask[j][1]):\n",
    "                temp_list.append(1)\n",
    "            else:\n",
    "                temp_list.append(0)           \n",
    "\n",
    "            # Since spans and mapping_list are ordered, break to allow it to catch up\n",
    "            if(spans_to_mask[j][1] > mapping_list[i][2]):\n",
    "                stop=True\n",
    "            else:\n",
    "                j = j+1\n",
    "            \n",
    "        if sum(temp_list) >= 1:\n",
    "            label_list.append(1)\n",
    "        else:\n",
    "            label_list.append(0)\n",
    "        \n",
    "    \n",
    "    return label_list  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53317ade",
   "metadata": {},
   "source": [
    "## Longformer\n",
    "### Create Labels and Tokenize Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "090afc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in tokenizer\n",
    "\n",
    "tokenizer = LongformerTokenizerFast.from_pretrained(\"allenai/longformer-base-4096\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dedf6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-23 14:45:46.907412: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-11-23 14:45:46.907579: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    }
   ],
   "source": [
    "# Create labels\n",
    "\n",
    "dev_text = []\n",
    "dev_labels = []\n",
    "\n",
    "for i in range(len(dev_file)):\n",
    "    doc_id = dev_file[i][\"doc_id\"]\n",
    "    spans_to_mask = dev_masks[doc_id]\n",
    "    spans_to_mask = list({tuple(x) for x in spans_to_mask}) # Make spans unique\n",
    "    doc_text = dev_file[i][\"text\"]\n",
    "    tok_tensor = tokenizer(doc_text, return_tensors=\"tf\", truncation=True, padding=True, return_offsets_mapping=True)\n",
    "    tokens = tok_tensor[\"input_ids\"].numpy()[0]\n",
    "    offsets = tok_tensor[\"offset_mapping\"].numpy()[0]\n",
    "    dev_text.append(doc_text)\n",
    "    dev_labels.append(label_tokens(tokens, offsets, spans_to_mask))\n",
    "    \n",
    "train_text = []\n",
    "train_labels = []\n",
    "\n",
    "for i in range(len(train_file)):\n",
    "    doc_id = train_file[i][\"doc_id\"]\n",
    "    spans_to_mask = train_masks[doc_id]\n",
    "    spans_to_mask = list({tuple(x) for x in spans_to_mask}) # Make spans unique\n",
    "    doc_text = train_file[i][\"text\"]\n",
    "    tok_tensor = tokenizer(doc_text, return_tensors=\"tf\", truncation=True, padding=True, return_offsets_mapping=True)\n",
    "    tokens = tok_tensor[\"input_ids\"].numpy()[0]\n",
    "    offsets = tok_tensor[\"offset_mapping\"].numpy()[0]\n",
    "    \n",
    "    train_text.append(doc_text)\n",
    "    train_labels.append(label_tokens(tokens, offsets, spans_to_mask))\n",
    "\n",
    "test_text = []\n",
    "test_labels = []\n",
    "\n",
    "for i in range(len(test_file)):\n",
    "    doc_id = test_file[i][\"doc_id\"]\n",
    "    spans_to_mask = test_masks[doc_id]\n",
    "    spans_to_mask = list({tuple(x) for x in spans_to_mask}) # Make spans unique\n",
    "    doc_text = test_file[i][\"text\"]\n",
    "    tok_tensor = tokenizer(doc_text, return_tensors=\"tf\", truncation=True, padding=True, return_offsets_mapping=True)\n",
    "    tokens = tok_tensor[\"input_ids\"].numpy()[0]\n",
    "    offsets = tok_tensor[\"offset_mapping\"].numpy()[0]\n",
    "    \n",
    "    test_text.append(doc_text)\n",
    "    test_labels.append(label_tokens(tokens, offsets, spans_to_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a93ccad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad labels to max length\n",
    "\n",
    "MAX_LEN = 4096\n",
    "\n",
    "for i in range(len(dev_labels)):\n",
    "    curr_len = len(dev_labels[i])\n",
    "    \n",
    "    if curr_len < MAX_LEN:\n",
    "        to_add = [0] * (MAX_LEN - curr_len)\n",
    "        dev_labels[i].extend(to_add)\n",
    "        \n",
    "for i in range(len(train_labels)):\n",
    "    curr_len = len(train_labels[i])\n",
    "    \n",
    "    if curr_len < MAX_LEN:\n",
    "        to_add = [0] * (MAX_LEN - curr_len)\n",
    "        train_labels[i].extend(to_add)\n",
    "        \n",
    "for i in range(len(test_labels)):\n",
    "    curr_len = len(test_labels[i])\n",
    "    \n",
    "    if curr_len < MAX_LEN:\n",
    "        to_add = [0] * (MAX_LEN - curr_len)\n",
    "        test_labels[i].extend(to_add)\n",
    "        \n",
    "dev_labels = np.asarray(dev_labels)\n",
    "train_labels = np.asarray(train_labels)\n",
    "test_labels = np.asarray(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1abf82d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize input\n",
    "\n",
    "dev_text_tokenized = tokenizer(dev_text, truncation=True, padding=\"max_length\", return_tensors=\"tf\")\n",
    "train_text_tokenized = tokenizer(train_text, truncation=True, padding=\"max_length\", return_tensors=\"tf\")\n",
    "test_text_tokenized = tokenizer(test_text, truncation=True, padding=\"max_length\", return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd0f0ec",
   "metadata": {},
   "source": [
    "## Specter\n",
    "### Create Labels and Tokenize Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04de5b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in tokenizer\n",
    "\n",
    "specter_tokenizer = AutoTokenizer.from_pretrained('allenai/specter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ac98939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "\n",
    "dev_text_tokenized_specter = specter_tokenizer(dev_text, truncation=True, max_length=4096, padding=\"max_length\", return_tensors=\"tf\")\n",
    "train_text_tokenized_specter = specter_tokenizer(train_text, truncation=True, max_length=4096, padding=\"max_length\", return_tensors=\"tf\")\n",
    "test_text_tokenized_specter = specter_tokenizer(test_text, truncation=True, max_length=4096, padding=\"max_length\", return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a386fa",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b5f1fe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ensemble_model(dropout=0.3,\n",
    "                            learning_rate=0.00002):\n",
    "    \n",
    "    \"\"\"Build FFNN on top of concatenated hidden states\"\"\"\n",
    "    \n",
    "    longformer_model = TFLongformerModel.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "    specter_model = TFAutoModel.from_pretrained('allenai/specter')\n",
    "    \n",
    "    longformer_model.trainable = True\n",
    "    specter_model.trainable = True\n",
    "    \n",
    "    input_ids_longformer = tf.keras.layers.Input(shape=(4096,), dtype=tf.int32, name=\"longformer_input\") # max_len for Longformer\n",
    "    input_ids_specter = tf.keras.layers.Input(shape=(4096,), dtype=tf.int32, name=\"specter_input\") # match length for Longformer\n",
    "    \n",
    "    longformer_out = longformer_model(input_ids_longformer).last_hidden_state\n",
    "    specter_out = specter_model(input_ids_specter).last_hidden_state\n",
    "    \n",
    "    concatenated_input = tf.concat([longformer_out, specter_out], axis=2)\n",
    "    \n",
    "    hidden_1 = tf.keras.layers.Dense(512, activation=\"relu\", name=\"hidden_1\")(concatenated_input)\n",
    "    hidden_1 = tf.keras.layers.Dropout(dropout)(hidden_1)\n",
    "    hidden_2 = tf.keras.layers.Dense(256, activation=\"relu\", name=\"hidden_2\")(hidden_1)\n",
    "    hidden_2 = tf.keras.layers.Dropout(dropout)(hidden_2)\n",
    "    \n",
    "    classification = tf.keras.layers.Dense(2, activation=\"sigmoid\", name=\"classification_layer\")(hidden_2)\n",
    "    \n",
    "    classification_model = tf.keras.Model(inputs=[input_ids_longformer, input_ids_specter], outputs=[classification])\n",
    "    \n",
    "    classification_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                                 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), \n",
    "                                 metrics=\"accuracy\")\n",
    "    \n",
    "    return classification_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f1181643",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at allenai/longformer-base-4096 were not used when initializing TFLongformerModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFLongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFLongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFLongformerModel were initialized from the model checkpoint at allenai/longformer-base-4096.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFLongformerModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFBertModel.\n",
      "\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at allenai/specter.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = create_ensemble_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250cd685",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-23 14:55:46.720077: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0', 'tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0', 'tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-23 14:56:00.229593: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-11-23 14:56:02.675056: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: model/tf_longformer_model/longformer/encoder/layer_._6/attention/self/cond_3/branch_executed/_732\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([train_text_tokenized[\"input_ids\"], train_text_tokenized_specter[\"input_ids\"]],\n",
    "                    train_labels,\n",
    "                    batch_size=1,\n",
    "                    epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b67e796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad90c71e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eea8e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e120905",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0b27c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b17d708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "\n",
    "model.save(\"../models/ensemble.h5\", save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545ba17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66f0123",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95843677",
   "metadata": {},
   "source": [
    "# Assess Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b09cea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(test_text_tokenized[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2904a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_token_class_ids = tf.math.argmax(preds, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7963efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_token_class_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5efad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predicted_token_class_ids to avoid running inference again\n",
    "\n",
    "np.savetxt(\"../predictions/longformer_ffnn_preds_tab_test_set.txt\", predicted_token_class_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec2c34c",
   "metadata": {},
   "source": [
    "## Wikipedia\n",
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa7ef28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load wiki data\n",
    "\n",
    "with open(\"../data/raw/wiki-summaries/annotated_wikipedia.json\") as file:\n",
    "    wiki_file = json.load(file)\n",
    "\n",
    "with open(\"../data/processed/wiki_masks.json\") as file:\n",
    "    wiki_masks = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e85a627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels\n",
    "\n",
    "wiki_text = []\n",
    "wiki_labels = []\n",
    "\n",
    "for i in range(len(wiki_file)):\n",
    "    doc_id = wiki_file[i][\"doc_id\"]\n",
    "    spans_to_mask = wiki_masks[doc_id]\n",
    "    spans_to_mask = list({tuple(x) for x in spans_to_mask}) # Make spans unique\n",
    "    doc_text = wiki_file[i][\"text\"]\n",
    "    tok_tensor = tokenizer(doc_text, return_tensors=\"tf\", truncation=True, padding=True, return_offsets_mapping=True)\n",
    "    tokens = tok_tensor[\"input_ids\"].numpy()[0]\n",
    "    offsets = tok_tensor[\"offset_mapping\"].numpy()[0]\n",
    "    wiki_text.append(doc_text)\n",
    "    wiki_labels.append(label_tokens(tokens, offsets, spans_to_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cd096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad labels to max length\n",
    "\n",
    "MAX_LEN = 4096\n",
    "\n",
    "for i in range(len(wiki_labels)):\n",
    "    curr_len = len(wiki_labels[i])\n",
    "    \n",
    "    if curr_len < MAX_LEN:\n",
    "        to_add = [0] * (MAX_LEN - curr_len)\n",
    "        wiki_labels[i].extend(to_add)\n",
    "        \n",
    "wiki_labels = np.asarray(wiki_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c460fe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize input\n",
    "\n",
    "wiki_text_tokenized = tokenizer(wiki_text, truncation=True, padding=\"max_length\", return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1935f4b",
   "metadata": {},
   "source": [
    "### Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca2c7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(wiki_text_tokenized[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb457d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_token_class_ids = tf.math.argmax(preds, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a2e7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_token_class_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75a86c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predicted_token_class_ids to avoid running inference again\n",
    "\n",
    "np.savetxt(\"../predictions/longformer_ffnn_preds.txt\", predicted_token_class_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750f92e2",
   "metadata": {},
   "source": [
    "# Calculate Precision, Recall, and AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fd340a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_precision(pred_list, label_list):\n",
    "    \"\"\"Calculates precision of batch of predictions\"\"\"\n",
    "    \n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    \n",
    "    for i in range(len(pred_list)):\n",
    "        for j in range(len(pred_list[i])):\n",
    "        \n",
    "            if pred_list[i][j] == 1:\n",
    "                if label_list[i][j] == 1:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    fp += 1\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "    return tp / (tp + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e53eae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_recall(pred_list, label_list):\n",
    "    \"\"\"Calculates recall of batch of predictions\"\"\"\n",
    "    \n",
    "    tp = 0 \n",
    "    fn = 0\n",
    "    \n",
    "    for i in range(len(pred_list)):\n",
    "        for j in range(len(pred_list[i])):\n",
    "            \n",
    "            if pred_list[i][j] == 1:\n",
    "                if label_list[i][j] == 1:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    tp += 0\n",
    "                \n",
    "            else:\n",
    "                if label_list[i][j] == 1:\n",
    "                    fn += 1\n",
    "                else:\n",
    "                    fn += 0\n",
    "    \n",
    "    return tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e186f1ee",
   "metadata": {},
   "source": [
    "## Test Set - Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5522c283",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = calc_precision(predicted_token_class_ids, test_labels)\n",
    "print (f' Token level precision: {precision}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ad97ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = calc_recall(predicted_token_class_ids, test_labels)\n",
    "print (f' Token level recall: {recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3908f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilabel indicators are not supported in sklearn for AUC\n",
    "# Loop through preds and take avg of AUC\n",
    "\n",
    "auc = []\n",
    "\n",
    "for i in range(len(predicted_token_class_ids)):\n",
    "    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(test_labels[i], predicted_token_class_ids[i], pos_label=1)\n",
    "    auc.append(metrics.auc(fpr, tpr))\n",
    "\n",
    "auc = sum(auc)/len(auc)\n",
    "print (f' Average AUC: {auc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f152014b",
   "metadata": {},
   "source": [
    "## Wikipedia - Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931828eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = calc_precision(predicted_token_class_ids, wiki_labels)\n",
    "print (f' Token level precision: {precision}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3bb6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = calc_recall(predicted_token_class_ids, wiki_labels)\n",
    "print (f' Token level recall: {recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae21e993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilabel indicators are not supported in sklearn for AUC\n",
    "# Loop through preds and take avg of AUC\n",
    "\n",
    "auc = []\n",
    "\n",
    "for i in range(len(predicted_token_class_ids)):\n",
    "    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(wiki_labels[i], predicted_token_class_ids[i], pos_label=1)\n",
    "    auc.append(metrics.auc(fpr, tpr))\n",
    "\n",
    "auc = sum(auc)/len(auc)\n",
    "print (f' Average AUC: {auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebc80a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
