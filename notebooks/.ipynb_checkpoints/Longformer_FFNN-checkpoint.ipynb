{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbd8188b",
   "metadata": {},
   "source": [
    "# Longformer with FFNN\n",
    "\n",
    "Instead of feeding hidden states from Longformer to a linear inference layer, feed them to a multilayer feedforward neural network\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "603e4761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import LongformerTokenizerFast, TFLongformerModel\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "from sklearn.metrics import auc, plot_precision_recall_curve, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dcefe2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at allenai/longformer-base-4096 were not used when initializing TFLongformerModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFLongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFLongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFLongformerModel were initialized from the model checkpoint at allenai/longformer-base-4096.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFLongformerModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Pull in tokenizer and model\n",
    "\n",
    "tokenizer = LongformerTokenizerFast.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "model = TFLongformerModel.from_pretrained(\"allenai/longformer-base-4096\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "46c86744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in data\n",
    "\n",
    "DEV_MASKS_FILE =    \"../data/processed/jg_dev_masks.json\"\n",
    "TRAIN_MASKS_FILE =  \"../data/processed/jg_train_masks.json\"\n",
    "TEST_MASKS_FILE =   \"../data/processed/jg_test_masks.json\"\n",
    "\n",
    "with open(\"../data/raw/text-anonymization-benchmark/echr_dev.json\") as file:\n",
    "    dev_file = json.load(file)\n",
    "\n",
    "with open(DEV_MASKS_FILE) as file:\n",
    "    dev_masks = json.load(file)\n",
    "    \n",
    "with open(\"../data/raw/text-anonymization-benchmark/echr_train.json\") as file:\n",
    "    train_file = json.load(file)\n",
    "    \n",
    "with open(TRAIN_MASKS_FILE) as file:\n",
    "    train_masks = json.load(file)\n",
    "       \n",
    "with open(\"../data/raw/text-anonymization-benchmark/echr_test.json\") as file:\n",
    "    test_file = json.load(file)\n",
    "\n",
    "with open(TEST_MASKS_FILE) as file:\n",
    "    test_masks = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195c8fc7",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "871c636e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to label data\n",
    "\n",
    "def label_tokens(toks, offs, spans_to_mask):\n",
    "    \"\"\"Args: \n",
    "            toks - list of token id's\n",
    "            offs - list of char offsets for each token\n",
    "       Returns:\n",
    "            label_list - 0 for non_mask, 1 for mask\"\"\"\n",
    "    \n",
    "    label_list = []\n",
    "    mapping_list = []\n",
    "    \n",
    "    # Map token_ids back to string\n",
    "    \n",
    "    for token, pos in zip(toks, offs):\n",
    "        mapping_list.append([token, pos[0], pos[1]])\n",
    "    \n",
    "    # Determine if each token should be masked\n",
    "    spans_to_mask.sort(key=lambda tup: tup[0]) #order spans, ascending\n",
    "    \n",
    "    j=0\n",
    "    \n",
    "    for i in range(len(mapping_list)):\n",
    "        \n",
    "        temp_list = []\n",
    "        stop=False\n",
    "        \n",
    "        while not stop and j < len(spans_to_mask):\n",
    "            \n",
    "            if (mapping_list[i][1] >= spans_to_mask[j][0]) and (mapping_list[i][2] <= spans_to_mask[j][1]):\n",
    "                temp_list.append(1)\n",
    "            else:\n",
    "                temp_list.append(0)           \n",
    "\n",
    "            # Since spans and mapping_list are ordered, break to allow it to catch up\n",
    "            if(spans_to_mask[j][1] > mapping_list[i][2]):\n",
    "                stop=True\n",
    "            else:\n",
    "                j = j+1\n",
    "            \n",
    "        if sum(temp_list) >= 1:\n",
    "            label_list.append(1)\n",
    "        else:\n",
    "            label_list.append(0)\n",
    "        \n",
    "    \n",
    "    return label_list  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53317ade",
   "metadata": {},
   "source": [
    "## Create Labels and Tokenize Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3dedf6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels\n",
    "\n",
    "dev_text = []\n",
    "dev_labels = []\n",
    "\n",
    "for i in range(len(dev_file)):\n",
    "    doc_id = dev_file[i][\"doc_id\"]\n",
    "    spans_to_mask = dev_masks[doc_id]\n",
    "    spans_to_mask = list({tuple(x) for x in spans_to_mask}) # Make spans unique\n",
    "    doc_text = dev_file[i][\"text\"]\n",
    "    tok_tensor = tokenizer(doc_text, return_tensors=\"tf\", truncation=True, padding=True, return_offsets_mapping=True)\n",
    "    tokens = tok_tensor[\"input_ids\"].numpy()[0]\n",
    "    offsets = tok_tensor[\"offset_mapping\"].numpy()[0]\n",
    "    dev_text.append(doc_text)\n",
    "    dev_labels.append(label_tokens(tokens, offsets, spans_to_mask))\n",
    "    \n",
    "train_text = []\n",
    "train_labels = []\n",
    "\n",
    "for i in range(len(train_file)):\n",
    "    doc_id = train_file[i][\"doc_id\"]\n",
    "    spans_to_mask = train_masks[doc_id]\n",
    "    spans_to_mask = list({tuple(x) for x in spans_to_mask}) # Make spans unique\n",
    "    doc_text = train_file[i][\"text\"]\n",
    "    tok_tensor = tokenizer(doc_text, return_tensors=\"tf\", truncation=True, padding=True, return_offsets_mapping=True)\n",
    "    tokens = tok_tensor[\"input_ids\"].numpy()[0]\n",
    "    offsets = tok_tensor[\"offset_mapping\"].numpy()[0]\n",
    "    \n",
    "    train_text.append(doc_text)\n",
    "    train_labels.append(label_tokens(tokens, offsets, spans_to_mask))\n",
    "\n",
    "test_text = []\n",
    "test_labels = []\n",
    "\n",
    "for i in range(len(test_file)):\n",
    "    doc_id = test_file[i][\"doc_id\"]\n",
    "    spans_to_mask = test_masks[doc_id]\n",
    "    spans_to_mask = list({tuple(x) for x in spans_to_mask}) # Make spans unique\n",
    "    doc_text = test_file[i][\"text\"]\n",
    "    tok_tensor = tokenizer(doc_text, return_tensors=\"tf\", truncation=True, padding=True, return_offsets_mapping=True)\n",
    "    tokens = tok_tensor[\"input_ids\"].numpy()[0]\n",
    "    offsets = tok_tensor[\"offset_mapping\"].numpy()[0]\n",
    "    \n",
    "    test_text.append(doc_text)\n",
    "    test_labels.append(label_tokens(tokens, offsets, spans_to_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a93ccad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad labels to max length\n",
    "\n",
    "MAX_LEN = 4096\n",
    "\n",
    "for i in range(len(dev_labels)):\n",
    "    curr_len = len(dev_labels[i])\n",
    "    \n",
    "    if curr_len < MAX_LEN:\n",
    "        to_add = [0] * (MAX_LEN - curr_len)\n",
    "        dev_labels[i].extend(to_add)\n",
    "        \n",
    "for i in range(len(train_labels)):\n",
    "    curr_len = len(train_labels[i])\n",
    "    \n",
    "    if curr_len < MAX_LEN:\n",
    "        to_add = [0] * (MAX_LEN - curr_len)\n",
    "        train_labels[i].extend(to_add)\n",
    "        \n",
    "for i in range(len(test_labels)):\n",
    "    curr_len = len(test_labels[i])\n",
    "    \n",
    "    if curr_len < MAX_LEN:\n",
    "        to_add = [0] * (MAX_LEN - curr_len)\n",
    "        test_labels[i].extend(to_add)\n",
    "        \n",
    "dev_labels = np.asarray(dev_labels)\n",
    "train_labels = np.asarray(train_labels)\n",
    "test_labels = np.asarray(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1abf82d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize input\n",
    "\n",
    "dev_text_tokenized = tokenizer(dev_text, truncation=True, padding=\"max_length\", return_tensors=\"tf\")\n",
    "train_text_tokenized = tokenizer(train_text, truncation=True, padding=\"max_length\", return_tensors=\"tf\")\n",
    "test_text_tokenized = tokenizer(test_text, truncation=True, padding=\"max_length\", return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a386fa",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b5f1fe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_longformer_model(dropout=0.3,\n",
    "                            learning_rate=0.00002):\n",
    "    \n",
    "    \"\"\"Build FFNN on top of Longformer hidden states\"\"\"\n",
    "    \n",
    "    longformer_model = TFLongformerModel.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "    longformer_model.trainable = True\n",
    "    \n",
    "    input_ids = tf.keras.layers.Input(shape=(4096,), dtype=tf.int32, name=\"input_ids_layer\") # max_len for Longformer\n",
    "    longformer_out = longformer_model(input_ids).last_hidden_state\n",
    "    \n",
    "    hidden_1 = tf.keras.layers.Dense(512, activation=\"relu\", name=\"hidden_1\")(longformer_out)\n",
    "    hidden_1 = tf.keras.layers.Dropout(dropout)(hidden_1)\n",
    "    hidden_2 = tf.keras.layers.Dense(256, activation=\"relu\", name=\"hidden_2\")(hidden_1)\n",
    "    hidden_2 = tf.keras.layers.Dropout(dropout)(hidden_2)\n",
    "    \n",
    "    classification = tf.keras.layers.Dense(2, activation=\"sigmoid\", name=\"classification_layer\")(hidden_2)\n",
    "    \n",
    "    classification_model = tf.keras.Model(inputs=[input_ids], outputs=[classification])\n",
    "    \n",
    "    classification_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                                 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), \n",
    "                                 metrics=\"accuracy\")\n",
    "    \n",
    "    return classification_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f1181643",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at allenai/longformer-base-4096 were not used when initializing TFLongformerModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFLongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFLongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFLongformerModel were initialized from the model checkpoint at allenai/longformer-base-4096.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFLongformerModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = create_longformer_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "250cd685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model_9/longformer/pooler/dense/kernel:0', 'tf_longformer_model_9/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model_9/longformer/pooler/dense/kernel:0', 'tf_longformer_model_9/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-12 22:59:42.116628: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: model_6/tf_longformer_model_9/longformer/encoder/layer_._9/attention/self/cond_2/branch_executed/_1009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1014/1014 [==============================] - 13704s 13s/step - loss: 0.0522 - accuracy: 0.9804\n",
      "Epoch 2/2\n",
      "1014/1014 [==============================] - 13623s 13s/step - loss: 0.0371 - accuracy: 0.9855\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_text_tokenized[\"input_ids\"],\n",
    "                    train_labels,\n",
    "                    batch_size=1,\n",
    "                    epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2b17d708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "\n",
    "model.save(\"../models/longformer_ffnn.h5\", save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "545ba17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [0.05220803618431091, 0.03706428408622742], 'accuracy': [0.9804208278656006, 0.9854947924613953]}\n"
     ]
    }
   ],
   "source": [
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e66f0123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_ids_layer (InputLayer  [(None, 4096)]           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " tf_longformer_model_9 (TFLo  TFLongformerBaseModelOut  148659456\n",
      " ngformerModel)              putWithPooling(last_hidd            \n",
      "                             en_state=(None, 4096, 76            \n",
      "                             8),                                 \n",
      "                              pooler_output=(None, 76            \n",
      "                             8),                                 \n",
      "                              hidden_states=None, att            \n",
      "                             entions=None, global_att            \n",
      "                             entions=None)                       \n",
      "                                                                 \n",
      " hidden_1 (Dense)            (None, 4096, 512)         393728    \n",
      "                                                                 \n",
      " dropout_501 (Dropout)       (None, 4096, 512)         0         \n",
      "                                                                 \n",
      " hidden_2 (Dense)            (None, 4096, 256)         131328    \n",
      "                                                                 \n",
      " dropout_502 (Dropout)       (None, 4096, 256)         0         \n",
      "                                                                 \n",
      " classification_layer (Dense  (None, 4096, 2)          514       \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 149,185,026\n",
      "Trainable params: 149,185,026\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95843677",
   "metadata": {},
   "source": [
    "# Assess Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2b09cea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(test_text_tokenized[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6a2904a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_token_class_ids = tf.math.argmax(preds, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b7963efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(127, 4096), dtype=int64, numpy=\n",
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_token_class_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d5efad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predicted_token_class_ids to avoid running inference again\n",
    "\n",
    "np.savetxt(\"../predictions/longformer_ffnn_preds_tab_test_set.txt\", predicted_token_class_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec2c34c",
   "metadata": {},
   "source": [
    "## Wikipedia\n",
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7fa7ef28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load wiki data\n",
    "\n",
    "with open(\"../data/raw/wiki-summaries/annotated_wikipedia.json\") as file:\n",
    "    wiki_file = json.load(file)\n",
    "\n",
    "with open(\"../data/processed/wiki_masks.json\") as file:\n",
    "    wiki_masks = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6e85a627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels\n",
    "\n",
    "wiki_text = []\n",
    "wiki_labels = []\n",
    "\n",
    "for i in range(len(wiki_file)):\n",
    "    doc_id = wiki_file[i][\"doc_id\"]\n",
    "    spans_to_mask = wiki_masks[doc_id]\n",
    "    spans_to_mask = list({tuple(x) for x in spans_to_mask}) # Make spans unique\n",
    "    doc_text = wiki_file[i][\"text\"]\n",
    "    tok_tensor = tokenizer(doc_text, return_tensors=\"tf\", truncation=True, padding=True, return_offsets_mapping=True)\n",
    "    tokens = tok_tensor[\"input_ids\"].numpy()[0]\n",
    "    offsets = tok_tensor[\"offset_mapping\"].numpy()[0]\n",
    "    wiki_text.append(doc_text)\n",
    "    wiki_labels.append(label_tokens(tokens, offsets, spans_to_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c1cd096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad labels to max length\n",
    "\n",
    "MAX_LEN = 4096\n",
    "\n",
    "for i in range(len(wiki_labels)):\n",
    "    curr_len = len(wiki_labels[i])\n",
    "    \n",
    "    if curr_len < MAX_LEN:\n",
    "        to_add = [0] * (MAX_LEN - curr_len)\n",
    "        wiki_labels[i].extend(to_add)\n",
    "        \n",
    "wiki_labels = np.asarray(wiki_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c460fe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize input\n",
    "\n",
    "wiki_text_tokenized = tokenizer(wiki_text, truncation=True, padding=\"max_length\", return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1935f4b",
   "metadata": {},
   "source": [
    "### Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "cca2c7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-13 08:52:40.950143: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 104375255040 exceeds 10% of free system memory.\n",
      "2022-11-13 08:53:37.473129: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 104579112960 exceeds 10% of free system memory.\n",
      "2022-11-13 08:53:53.126663: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 104579112960 exceeds 10% of free system memory.\n",
      "2022-11-13 08:55:34.232036: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 83717652480 exceeds 10% of free system memory.\n",
      "2022-11-13 08:55:39.582864: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 83608928256 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "preds = model(wiki_text_tokenized[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "eb457d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_token_class_ids = tf.math.argmax(preds, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "89a2e7d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(553, 4096), dtype=int64, numpy=\n",
       "array([[0, 1, 1, ..., 0, 0, 0],\n",
       "       [0, 1, 1, ..., 0, 0, 0],\n",
       "       [0, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 1, ..., 0, 0, 0],\n",
       "       [0, 1, 1, ..., 0, 0, 0]])>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_token_class_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c75a86c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predicted_token_class_ids to avoid running inference again\n",
    "\n",
    "np.savetxt(\"../predictions/longformer_ffnn_preds.txt\", predicted_token_class_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750f92e2",
   "metadata": {},
   "source": [
    "# Calculate Precision, Recall, and AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "56fd340a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_precision(pred_list, label_list):\n",
    "    \"\"\"Calculates precision of batch of predictions\"\"\"\n",
    "    \n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    \n",
    "    for i in range(len(pred_list)):\n",
    "        for j in range(len(pred_list[i])):\n",
    "        \n",
    "            if pred_list[i][j] == 1:\n",
    "                if label_list[i][j] == 1:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    fp += 1\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "    return tp / (tp + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2e53eae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_recall(pred_list, label_list):\n",
    "    \"\"\"Calculates recall of batch of predictions\"\"\"\n",
    "    \n",
    "    tp = 0 \n",
    "    fn = 0\n",
    "    \n",
    "    for i in range(len(pred_list)):\n",
    "        for j in range(len(pred_list[i])):\n",
    "            \n",
    "            if pred_list[i][j] == 1:\n",
    "                if label_list[i][j] == 1:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    tp += 0\n",
    "                \n",
    "            else:\n",
    "                if label_list[i][j] == 1:\n",
    "                    fn += 1\n",
    "                else:\n",
    "                    fn += 0\n",
    "    \n",
    "    return tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e186f1ee",
   "metadata": {},
   "source": [
    "## Test Set - Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5522c283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Token level precision: 0.9460919353967482\n"
     ]
    }
   ],
   "source": [
    "precision = calc_precision(predicted_token_class_ids, test_labels)\n",
    "print (f' Token level precision: {precision}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "75ad97ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Token level recall: 0.7401850990998605\n"
     ]
    }
   ],
   "source": [
    "recall = calc_recall(predicted_token_class_ids, test_labels)\n",
    "print (f' Token level recall: {recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e3908f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Average AUC: 0.8846555526884508\n"
     ]
    }
   ],
   "source": [
    "# Multilabel indicators are not supported in sklearn for AUC\n",
    "# Loop through preds and take avg of AUC\n",
    "\n",
    "auc = []\n",
    "\n",
    "for i in range(len(predicted_token_class_ids)):\n",
    "    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(test_labels[i], predicted_token_class_ids[i], pos_label=1)\n",
    "    auc.append(metrics.auc(fpr, tpr))\n",
    "\n",
    "auc = sum(auc)/len(auc)\n",
    "print (f' Average AUC: {auc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f152014b",
   "metadata": {},
   "source": [
    "## Wikipedia - Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "931828eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Token level precision: 0.9033591485722431\n"
     ]
    }
   ],
   "source": [
    "precision = calc_precision(predicted_token_class_ids, wiki_labels)\n",
    "print (f' Token level precision: {precision}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "9d3bb6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Token level recall: 0.7630533370791026\n"
     ]
    }
   ],
   "source": [
    "recall = calc_recall(predicted_token_class_ids, wiki_labels)\n",
    "print (f' Token level recall: {recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "ae21e993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Average AUC: 0.8875962720281091\n"
     ]
    }
   ],
   "source": [
    "# Multilabel indicators are not supported in sklearn for AUC\n",
    "# Loop through preds and take avg of AUC\n",
    "\n",
    "auc = []\n",
    "\n",
    "for i in range(len(predicted_token_class_ids)):\n",
    "    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(wiki_labels[i], predicted_token_class_ids[i], pos_label=1)\n",
    "    auc.append(metrics.auc(fpr, tpr))\n",
    "\n",
    "auc = sum(auc)/len(auc)\n",
    "print (f' Average AUC: {auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebc80a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
