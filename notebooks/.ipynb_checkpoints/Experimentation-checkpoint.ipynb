{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d97a0abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import LongformerTokenizerFast, TFLongformerModel, TFBertModel, LongformerConfig, BertConfig, TFBertTokenizer\n",
    "\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21c2397a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in data\n",
    "\n",
    "DEV_MASKS_FILE =    \"../data/processed/jg_dev_masks.json\"\n",
    "TRAIN_MASKS_FILE =  \"../data/processed/jg_train_masks.json\"\n",
    "TEST_MASKS_FILE =   \"../data/processed/jg_test_masks.json\"\n",
    "\n",
    "with open(\"../data/raw/text-anonymization-benchmark/echr_dev.json\") as file:\n",
    "    dev_file = json.load(file)\n",
    "\n",
    "with open(DEV_MASKS_FILE) as file:\n",
    "    dev_masks = json.load(file)\n",
    "    \n",
    "with open(\"../data/raw/text-anonymization-benchmark/echr_train.json\") as file:\n",
    "    train_file = json.load(file)\n",
    "    \n",
    "with open(TRAIN_MASKS_FILE) as file:\n",
    "    train_masks = json.load(file)\n",
    "       \n",
    "with open(\"../data/raw/text-anonymization-benchmark/echr_test.json\") as file:\n",
    "    test_file = json.load(file)\n",
    "\n",
    "with open(TEST_MASKS_FILE) as file:\n",
    "    test_masks = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab24c4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to label data\n",
    "\n",
    "def label_tokens(toks, offs, spans_to_mask):\n",
    "    \"\"\"Args: \n",
    "            toks - list of token id's\n",
    "            offs - list of char offsets for each token\n",
    "       Returns:\n",
    "            label_list - 0 for non_mask, 1 for mask\"\"\"\n",
    "    \n",
    "    label_list = []\n",
    "    mapping_list = []\n",
    "    \n",
    "    # Map token_ids back to string\n",
    "    \n",
    "    for token, pos in zip(toks, offs):\n",
    "        mapping_list.append([token, pos[0], pos[1]])\n",
    "    \n",
    "    # Determine if each token should be masked\n",
    "    spans_to_mask.sort(key=lambda tup: tup[0]) #order spans, ascending\n",
    "    \n",
    "    j=0\n",
    "    \n",
    "    for i in range(len(mapping_list)):\n",
    "        \n",
    "        temp_list = []\n",
    "        stop=False\n",
    "        \n",
    "        while not stop and j < len(spans_to_mask):\n",
    "            \n",
    "            if (mapping_list[i][1] >= spans_to_mask[j][0]) and (mapping_list[i][2] <= spans_to_mask[j][1]):\n",
    "                temp_list.append(1)\n",
    "            else:\n",
    "                temp_list.append(0)           \n",
    "\n",
    "            # Since spans and mapping_list are ordered, break to allow it to catch up\n",
    "            if(spans_to_mask[j][1] > mapping_list[i][2]):\n",
    "                stop=True\n",
    "            else:\n",
    "                j = j+1\n",
    "            \n",
    "        if sum(temp_list) >= 1:\n",
    "            label_list.append(1)\n",
    "        else:\n",
    "            label_list.append(0)\n",
    "        \n",
    "    \n",
    "    return label_list  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f9e590",
   "metadata": {},
   "source": [
    "# Prepare example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b33e94af",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nTFBertTokenizer requires the tensorflow_text library but it was not found in your environment. You can install it with pip as\nexplained here: https://www.tensorflow.org/text/guide/tf_text_intro.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xt/wkt538cx4tbf_ssfzkgm44z40000gn/T/ipykernel_27996/4260334673.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlong_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLongformerTokenizerFast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"allenai/longformer-base-4096\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbert_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTFBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-base-cased\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m         \u001b[0mrequires_backends\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backends\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m    904\u001b[0m     \u001b[0mfailed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mavailable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchecks\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mavailable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfailed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 906\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfailed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    907\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: \nTFBertTokenizer requires the tensorflow_text library but it was not found in your environment. You can install it with pip as\nexplained here: https://www.tensorflow.org/text/guide/tf_text_intro.\n"
     ]
    }
   ],
   "source": [
    "long_tokenizer = LongformerTokenizerFast.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "bert_tokenizer = TFBertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f2b2215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels\n",
    "\n",
    "dev_text = []\n",
    "dev_labels = []\n",
    "\n",
    "for i in range(len(dev_file)):\n",
    "    doc_id = dev_file[i][\"doc_id\"]\n",
    "    spans_to_mask = dev_masks[doc_id]\n",
    "    spans_to_mask = list({tuple(x) for x in spans_to_mask}) # Make spans unique\n",
    "    doc_text = dev_file[i][\"text\"]\n",
    "    tok_tensor = long_tokenizer(doc_text, return_tensors=\"tf\", truncation=True, padding=True, return_offsets_mapping=True)\n",
    "    tokens = tok_tensor[\"input_ids\"].numpy()[0]\n",
    "    offsets = tok_tensor[\"offset_mapping\"].numpy()[0]\n",
    "    dev_text.append(doc_text)\n",
    "    dev_labels.append(label_tokens(tokens, offsets, spans_to_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "362e7754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad labels to max length\n",
    "\n",
    "MAX_LEN = 4096\n",
    "\n",
    "for i in range(len(dev_labels)):\n",
    "    curr_len = len(dev_labels[i])\n",
    "    \n",
    "    if curr_len < MAX_LEN:\n",
    "        to_add = [0] * (MAX_LEN - curr_len)\n",
    "        dev_labels[i].extend(to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d4cb635",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_labels = np.asarray(dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3fe489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_text_tokenized = long_tokenizer(dev_text, truncation=True, padding=\"max_length\", return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95eb0039",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_input = dev_text_tokenized[\"input_ids\"][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a193f9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = LongformerConfig(hidden_size=768)\n",
    "model = TFLongformerModel(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9a2624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_out = model(long_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a73449dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 4096, 768), dtype=float32, numpy=\n",
       "array([[[ 1.2120625 , -0.05847552, -1.6469035 , ..., -1.4357332 ,\n",
       "         -0.09620433,  0.04101687],\n",
       "        [ 1.210464  , -0.0719308 , -3.7255435 , ..., -1.2456849 ,\n",
       "          0.5807308 ,  0.02675109],\n",
       "        [ 0.49896082, -0.6654972 , -1.4830106 , ..., -2.4679806 ,\n",
       "         -0.53557163,  0.7778685 ],\n",
       "        ...,\n",
       "        [ 1.1300122 , -1.8451174 , -0.6084656 , ..., -1.35957   ,\n",
       "          0.71025735,  0.59018344],\n",
       "        [ 1.1300173 , -1.8451171 , -0.6084657 , ..., -1.3595695 ,\n",
       "          0.7102577 ,  0.5901856 ],\n",
       "        [ 1.1300141 , -1.8451166 , -0.60846573, ..., -1.3595701 ,\n",
       "          0.7102593 ,  0.5901875 ]],\n",
       "\n",
       "       [[ 1.1743386 , -0.09056882, -1.6945956 , ..., -1.4370364 ,\n",
       "         -0.10498749,  0.04563296],\n",
       "        [ 1.1866105 , -0.09132639, -3.768911  , ..., -1.2406118 ,\n",
       "          0.5697771 ,  0.04183936],\n",
       "        [ 0.46857306, -0.687666  , -1.527316  , ..., -2.4570878 ,\n",
       "         -0.5353244 ,  0.79489774],\n",
       "        ...,\n",
       "        [ 1.1300122 , -1.8451174 , -0.6084656 , ..., -1.35957   ,\n",
       "          0.71025735,  0.59018344],\n",
       "        [ 1.1300173 , -1.8451171 , -0.6084657 , ..., -1.3595695 ,\n",
       "          0.7102577 ,  0.5901856 ],\n",
       "        [ 1.1300141 , -1.8451166 , -0.60846573, ..., -1.3595701 ,\n",
       "          0.7102593 ,  0.5901875 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will be concatenated with bert_out\n",
    "\n",
    "long_out.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e43b68fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of tokens in TAB dev set:  1146.1574803149606\n"
     ]
    }
   ],
   "source": [
    "run_sum = 0\n",
    "for i in range(len(dev_text_tokenized[\"attention_mask\"])):\n",
    "    run_sum += dev_text_tokenized[\"attention_mask\"][i].numpy().sum()\n",
    "    \n",
    "print (\"Average number of tokens in TAB dev set: \", run_sum/127)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29358b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f522a11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bad309",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52dfdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs_long = model(input_ids=None, inputs_embeds=embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3b30f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da987851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f432cf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce956aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271ed210",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17b12787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4eb90083",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = np.random.rand(2, 4096, 780)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b85630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f8d2be7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFLongformerBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor: shape=(2, 4096, 780), dtype=float32, numpy=\n",
       "array([[[-0.49232626, -0.559248  , -1.2002027 , ...,  0.2702738 ,\n",
       "          0.14512879,  2.6727633 ],\n",
       "        [-0.5254935 ,  0.82954466,  1.1130804 , ...,  0.9501388 ,\n",
       "         -0.35745513,  2.556387  ],\n",
       "        [-0.7506366 ,  0.82961226, -1.3330721 , ...,  0.56662714,\n",
       "          2.1735842 ,  1.2026347 ],\n",
       "        ...,\n",
       "        [-0.20581056, -0.08502284, -0.6277318 , ..., -0.35515654,\n",
       "          1.5830257 ,  0.5219814 ],\n",
       "        [ 0.8646531 , -0.90375865, -2.0135632 , ..., -0.3482434 ,\n",
       "         -0.67312104,  1.8517795 ],\n",
       "        [ 1.2273629 ,  1.0311047 ,  0.33809602, ...,  0.9056321 ,\n",
       "         -1.1252242 ,  1.626058  ]],\n",
       "\n",
       "       [[ 1.2560356 , -1.0362701 , -1.2846804 , ...,  0.25536045,\n",
       "          1.4691763 ,  2.2017922 ],\n",
       "        [ 1.778084  ,  0.07749991, -0.58557487, ..., -0.41259348,\n",
       "         -0.08013434,  1.694983  ],\n",
       "        [ 1.2428657 ,  0.98604006, -0.7024545 , ...,  1.3977537 ,\n",
       "         -1.3098955 , -0.72969854],\n",
       "        ...,\n",
       "        [-0.2881931 , -0.6036164 ,  0.09313399, ..., -0.1661211 ,\n",
       "          1.345043  ,  1.6775403 ],\n",
       "        [-0.35896313, -1.5677444 , -1.1860567 , ...,  0.6951119 ,\n",
       "         -0.07989793, -0.24983498],\n",
       "        [-0.3458694 ,  0.14366467, -1.5160263 , ..., -0.41490647,\n",
       "          0.84664565, -0.333223  ]]], dtype=float32)>, pooler_output=<tf.Tensor: shape=(2, 780), dtype=float32, numpy=\n",
       "array([[-0.29997447, -0.63401437, -0.30829653, ..., -0.38760307,\n",
       "         0.10287247, -0.5020018 ],\n",
       "       [ 0.6739492 , -0.36142445, -0.0956092 , ..., -0.48292765,\n",
       "         0.44818538, -0.48323926]], dtype=float32)>, hidden_states=None, attentions=None, global_attentions=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_ids=None, inputs_embeds=embeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7210b7d",
   "metadata": {},
   "source": [
    "# To get Longformer to accept and use different dimensional embeddings,\n",
    "\n",
    "The dimension of the hidden_size needs to be a multiple of 12 and\n",
    "You need to update the config class as above"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
