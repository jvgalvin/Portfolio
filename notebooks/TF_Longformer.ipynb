{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "331cdd9c",
   "metadata": {},
   "source": [
    "# Baseline: Longformer\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c61ca821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import LongformerTokenizerFast, TFLongformerForTokenClassification\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "from sklearn.metrics import auc, plot_precision_recall_curve, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ab617ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-12 17:35:18.108094: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some layers from the model checkpoint at allenai/longformer-base-4096 were not used when initializing TFLongformerForTokenClassification: ['lm_head']\n",
      "- This IS expected if you are initializing TFLongformerForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFLongformerForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFLongformerForTokenClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Pull in tokenizer and model\n",
    "\n",
    "tokenizer = LongformerTokenizerFast.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "model = TFLongformerForTokenClassification.from_pretrained(\"allenai/longformer-base-4096\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5faa502b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in data\n",
    "\n",
    "DEV_MASKS_FILE =    \"../data/processed/jg_dev_masks.json\"\n",
    "TRAIN_MASKS_FILE =  \"../data/processed/jg_train_masks.json\"\n",
    "TEST_MASKS_FILE =   \"../data/processed/jg_test_masks.json\"\n",
    "\n",
    "with open(\"../data/raw/text-anonymization-benchmark/echr_dev.json\") as file:\n",
    "    dev_file = json.load(file)\n",
    "\n",
    "with open(DEV_MASKS_FILE) as file:\n",
    "    dev_masks = json.load(file)\n",
    "    \n",
    "with open(\"../data/raw/text-anonymization-benchmark/echr_train.json\") as file:\n",
    "    train_file = json.load(file)\n",
    "    \n",
    "with open(TRAIN_MASKS_FILE) as file:\n",
    "    train_masks = json.load(file)\n",
    "       \n",
    "with open(\"../data/raw/text-anonymization-benchmark/echr_test.json\") as file:\n",
    "    test_file = json.load(file)\n",
    "\n",
    "with open(TEST_MASKS_FILE) as file:\n",
    "    test_masks = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbcfdd8",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28a8a865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to label data\n",
    "\n",
    "def label_tokens(toks, offs, spans_to_mask):\n",
    "    \"\"\"Args: \n",
    "            toks - list of token id's\n",
    "            offs - list of char offsets for each token\n",
    "       Returns:\n",
    "            label_list - 0 for non_mask, 1 for mask\"\"\"\n",
    "    \n",
    "    label_list = []\n",
    "    mapping_list = []\n",
    "    \n",
    "    # Map token_ids back to string\n",
    "    \n",
    "    for token, pos in zip(toks, offs):\n",
    "        mapping_list.append([token, pos[0], pos[1]])\n",
    "    \n",
    "    # Determine if each token should be masked\n",
    "    spans_to_mask.sort(key=lambda tup: tup[0]) #order spans, ascending\n",
    "    \n",
    "    j=0\n",
    "    \n",
    "    for i in range(len(mapping_list)):\n",
    "        \n",
    "        temp_list = []\n",
    "        stop=False\n",
    "        \n",
    "        while not stop and j < len(spans_to_mask):\n",
    "            \n",
    "            if (mapping_list[i][1] >= spans_to_mask[j][0]) and (mapping_list[i][2] <= spans_to_mask[j][1]):\n",
    "                temp_list.append(1)\n",
    "            else:\n",
    "                temp_list.append(0)           \n",
    "\n",
    "            # Since spans and mapping_list are ordered, break to allow it to catch up\n",
    "            if(spans_to_mask[j][1] > mapping_list[i][2]):\n",
    "                stop=True\n",
    "            else:\n",
    "                j = j+1\n",
    "            \n",
    "        if sum(temp_list) >= 1:\n",
    "            label_list.append(1)\n",
    "        else:\n",
    "            label_list.append(0)\n",
    "        \n",
    "    \n",
    "    return label_list  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e1dd1d",
   "metadata": {},
   "source": [
    "## Validate Data Labeling Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4438030f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_label_tokens (__main__.TestNotebook) ... ok\n",
      "test_spans_to_mask (__main__.TestNotebook) ... ok\n",
      "test_tokenizer (__main__.TestNotebook) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.026s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7fb1886bb850>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestNotebook(unittest.TestCase):\n",
    "\n",
    "    def test_spans_to_mask(self):\n",
    "        with open(DEV_MASKS_FILE) as file:\n",
    "            dev_masks = json.load(file)\n",
    "        spans_to_mask = dev_masks[\"001-83927\"]\n",
    "        spans_to_mask = list({tuple(x) for x in spans_to_mask}) # Make spans unique\n",
    "        expected_spans_to_mask = [(2920, 2935), (343, 354), (3010, 3027), (2218, 2236), (254, 269), (1561, 1569), (516, 531), (2422, 2432), (803, 807), (2073, 2085), (841, 861), (1100, 1112), (292, 307), (1212, 1228), (1100, 1206), (3215, 3229), (1468, 1480), (1561, 1586), (3076, 3089), (379, 387), (867, 882), (1118, 1205), (2202, 2213), (3094, 3112), (2179, 2196), (54, 62)]\n",
    "\n",
    "        #does equality, does not care about order\n",
    "        self.assertCountEqual(spans_to_mask, expected_spans_to_mask)\n",
    "\n",
    "    def test_tokenizer(self):\n",
    "        doc_text = \"PROCEDURE\\n\\nThe case originated in an application (no. 40593/04) against the Republic of Turkey lodged with the Court under Article 34 of the Convention for the Protection of Human Rights and Fundamental Freedoms (“the Convention”) by a Turkish national, Mr Cengiz Polat (“the applicant”), on 15 October 2004.\\n\\nThe applicant was represented by Mr E. Kanar, a lawyer practising in Istanbul. The Turkish Government (“the Government”) did not designate an Agent for the purposes of the proceedings before the Court.\\n\\nOn 6 November 2006 the Court decided to give notice of the application to the Government. Applying Article 29 § 3 of the Convention, it decided to rule on the admissibility and merits of the application at the same time.\\n\\nTHE FACTS\\n\\nTHE CIRCUMSTANCES OF THE CASE\\n\\nThe applicant was born in 1965. He is currently detained in the Edirne F-type Prison.\\n\\nOn 6 February 1993 the applicant was arrested and placed in police custody by officers from the Anti-terror branch of the Istanbul Security Directorate on suspicion of involvement in the activities of an illegal armed organisation, the TKP/ML-TIKKO (the Turkish Communist Party/Marxist Leninist -Turkish Workers and Peasants' Liberation Army).\\n\\nOn 15 February 1993 the applicant was brought before the public prosecutor and then the investigating judge at the Istanbul State Security Court. On the same day the investigating judge remanded the applicant in custody pending trial.\\n\\nBy an indictment dated 5 April 1993, the public prosecutor initiated criminal proceedings against the applicant and nineteen other defendants before the Istanbul State Security Court, accusing them, inter alia, of membership of an illegal armed organisation and of involvement in activities which undermined the constitutional order of the State. The prosecution sought the death penalty under Article 146 § 1 of the Criminal Code.\\n\\nIn the course of the proceedings, the State Security Court rejected the applicant's requests for release, taking into account the nature of the alleged offence and the state of the evidence.\\n\\nOn 12 June 2000 the applicant was convicted as charged by the Istanbul State Security Court and sentenced to life imprisonment.\\n\\nOn 15 May 2001 the Court of Cassation quashed the applicant's conviction for procedural reasons. The case was remitted to the Istanbul State Security Court for further examination and the applicant remained in custody.\\n\\nOn 7 May 2004 State Security Courts were abolished following a constitutional amendment and the applicant's case was transmitted to the Istanbul Assize Court. In the course of the proceedings, the domestic courts rejected the applicant's requests for release, taking into account the nature of the alleged offence and the documents in the case file. The applicant challenged these decisions under Article 298 of the Criminal Procedure Code; however, the domestic courts rejected all his requests.\\n\\nOn 31 January 2005 the Istanbul Assize Court found the applicant guilty and sentenced him to life imprisonment under Article 146 § 1 of the Criminal Code.\\n\\nOn 20 March 2006 the Court of Cassation quashed the decision of the Assize Court and the case file was remitted to Istanbul Assize Court.\\n\\nOn 4 October 2006 the applicant was released pending trial. According to the information in the case file, as submitted by the parties, the case is still pending before the Istanbul Assize Court.\"\n",
    "        tok_tensor = tokenizer(doc_text, return_tensors=\"tf\", padding=True, return_offsets_mapping=True)\n",
    "        tokens = tok_tensor[\"input_ids\"].numpy()[0]\n",
    "        offsets = tok_tensor[\"offset_mapping\"].numpy()[0]\n",
    "        tokens_expected = np.array([    0,  4454,  4571,  1691, 12435, 50118, 50118,   133,   403, 19575,    11,    41,  2502,    36,  2362,     4,   843, 39785,    73,  3387,    43,   136,     5,  3497,     9,  2769, 15434,    19,     5,   837,   223,  6776,  2631,     9,     5,  9127,    13,     5,  5922,     9,  3861,  3941,     8, 37898, 28000,  6806,    36,    17,    48,   627,  9127,    17,    46,    43,    30,    10,  4423,   632,     6,   427,   230,  3314,  1210,  6189,   415,    36,    17,    48,   627, 20321,    17,    46,   238,    15,   379,   779,  4482,     4, 50118, 50118,   133, 20321,    21,  4625,    30,   427,   381,     4,  7542,   271,     6,    10,  2470, 21886,  3009,    11, 12275,     4,    20,  4423,  1621,    36,    17,    48,   627,  1621,    17,    46,    43,   222,    45, 31815,    41, 18497,    13,     5,  6216,     9,     5,  7069,   137,     5,   837,     4, 50118, 50118,  4148,   231,   759,  3503,     5,   837,  1276,     7,   492,  3120,     9,     5,  2502,     7,     5,  1621,     4,  3166, 13010,  6776,  1132, 39207,   155,     9,     5,  9127,     6,    24,  1276,     7,  2178,    15,     5,  2329, 17745, 12203,     8, 20273,     9,     5,  2502,    23,     5,   276,    86,     4, 50118, 50118, 13354, 26207,  2685, 50118, 50118, 13354,   230, 44160,  5725,  4014,  9363,  1723,  3243,  1941, 39743, 50118, 50118,   133, 20321,    21,  2421,    11, 18202,     4,    91,    16,   855,  5624,    11,     5,  2344,   853,   858,   274,    12, 12528, 15591,     4, 50118, 50118,  4148,   231,   902,  9095,     5, 20321,    21,  1128,     8,  2325,    11,   249,  3469,    30,  1024,    31,     5,  9511,    12, 26213,  6084,     9,     5, 12275,  2010, 23691,    15,  8551,     9,  5292,    11,     5,  1713,     9,    41,  2439,  3234,  6010,     6,     5,   255,   530,   510,    73, 10537,    12, 13216, 26228,   673,    36,   627,  4423, 12416,  1643,    73, 44849,   661, 37632,   661,   111, 37300, 10586,     8,  4119,   281,  3277,   108, 22499,  2938,   322, 50118, 50118,  4148,   379,   902,  9095,     5, 20321,    21,  1146,   137,     5,   285,  5644,     8,   172,     5,  3219,  1679,    23,     5, 12275,   331,  2010,   837,     4,   374,     5,   276,   183,     5,  3219,  1679,  6398, 13833,     5, 20321,    11,  3469,  5319,  1500,     4, 50118, 50118,  2765,    41,  9645,  7000,   195,   587,  9095,     6,     5,   285,  5644,  9608,  1837,  7069,   136,     5, 20321,     8, 40126,    97,  9483,   137,     5, 12275,   331,  2010,   837,     6,  8601,   106,     6,  3222,  1076,   493,     6,     9,  6332,     9,    41,  2439,  3234,  6010,     8,     9,  5292,    11,  1713,    61, 21167,     5,  6100,   645,     9,     5,   331,     4,    20,  6914,  2952,     5,   744,  2861,   223,  6776, 24543, 39207,   112,     9,     5, 10203,  8302,     4, 50118, 50118,  1121,     5,   768,     9,     5,  7069,     6,     5,   331,  2010,   837,  3946,     5, 20321,    18,  5034,    13,   800,     6,   602,    88,  1316,     5,  2574,     9,     5,  1697,  8637,     8,     5,   194,     9,     5,  1283,     4, 50118, 50118,  4148,   316,   502,  3788,     5, 20321,    21,  3828,    25,  1340,    30,     5, 12275,   331,  2010,   837,     8,  4018,     7,   301, 14804,     4, 50118, 50118,  4148,   379,   392,  5155,     5,   837,     9, 11710,  1258,  2677,  9512,     5, 20321,    18,  6866,    13, 24126,  2188,     4,    20,   403,    21,  6398, 16430,     7,     5, 12275,   331,  2010,   837,    13,   617,  9027,     8,     5, 20321,  2442,    11,  3469,     4, 50118, 50118,  4148,   262,   392,  4482,   331,  2010, 25627,    58, 33110,   511,    10,  6100,  8322,     8,     5, 20321,    18,   403,    21, 20579,     7,     5, 12275,  6331,  2072,   837,     4,    96,     5,   768,     9,     5,  7069,     6,     5,  1897,  4354,  3946,     5, 20321,    18,  5034,    13,   800,     6,   602,    88,  1316,     5,  2574,     9,     5,  1697,  8637,     8,     5,  2339,    11,     5,   403,  2870,     4,    20, 20321,  6835,   209,  2390,   223,  6776, 37353,     9,     5, 10203, 40209,  8302,   131,   959,     6,     5,  1897,  4354,  3946,    70,    39,  5034,     4, 50118, 50118,  4148,  1105,   644,  4013,     5, 12275,  6331,  2072,   837,   303,     5, 20321,  2181,     8,  4018,   123,     7,   301, 14804,   223,  6776, 24543, 39207,   112,     9,     5, 10203,  8302,     4, 50118, 50118,  4148,   291,   494,  3503,     5,   837,     9, 11710,  1258,  2677,  9512,     5,   568,     9,     5,  6331,  2072,   837,     8,     5,   403,  2870,    21,  6398, 16430,     7, 12275,  6331,  2072,   837,     4, 50118, 50118,  4148,   204,   779,  3503,     5, 20321,    21,   703,  5319,  1500,     4,   767,     7,     5,   335,    11,     5,   403,  2870,     6,    25,  4813,    30,     5,  1799,     6,     5,   403,    16,   202,  5319,   137,     5, 12275,  6331,  2072,   837,     4,     2])\n",
    "    \n",
    "        #self.assertEqual(tokens, tokens_expected, \"Tokenizer is producing different results than expected.\")\n",
    "        self.assertTrue((tokens==tokens_expected).all())\n",
    "    \n",
    "    def test_label_tokens(self):\n",
    "        doc_text = \"PROCEDURE\\n\\nThe case originated in an application (no. 40593/04) against the Republic of Turkey lodged with the Court under Article 34 of the Convention for the Protection of Human Rights and Fundamental Freedoms (“the Convention”) by a Turkish national, Mr Cengiz Polat (“the applicant”), on 15 October 2004.\\n\\nThe applicant was represented by Mr E. Kanar, a lawyer practising in Istanbul. The Turkish Government (“the Government”) did not designate an Agent for the purposes of the proceedings before the Court.\\n\\nOn 6 November 2006 the Court decided to give notice of the application to the Government. Applying Article 29 § 3 of the Convention, it decided to rule on the admissibility and merits of the application at the same time.\\n\\nTHE FACTS\\n\\nTHE CIRCUMSTANCES OF THE CASE\\n\\nThe applicant was born in 1965. He is currently detained in the Edirne F-type Prison.\\n\\nOn 6 February 1993 the applicant was arrested and placed in police custody by officers from the Anti-terror branch of the Istanbul Security Directorate on suspicion of involvement in the activities of an illegal armed organisation, the TKP/ML-TIKKO (the Turkish Communist Party/Marxist Leninist -Turkish Workers and Peasants' Liberation Army).\\n\\nOn 15 February 1993 the applicant was brought before the public prosecutor and then the investigating judge at the Istanbul State Security Court. On the same day the investigating judge remanded the applicant in custody pending trial.\\n\\nBy an indictment dated 5 April 1993, the public prosecutor initiated criminal proceedings against the applicant and nineteen other defendants before the Istanbul State Security Court, accusing them, inter alia, of membership of an illegal armed organisation and of involvement in activities which undermined the constitutional order of the State. The prosecution sought the death penalty under Article 146 § 1 of the Criminal Code.\\n\\nIn the course of the proceedings, the State Security Court rejected the applicant's requests for release, taking into account the nature of the alleged offence and the state of the evidence.\\n\\nOn 12 June 2000 the applicant was convicted as charged by the Istanbul State Security Court and sentenced to life imprisonment.\\n\\nOn 15 May 2001 the Court of Cassation quashed the applicant's conviction for procedural reasons. The case was remitted to the Istanbul State Security Court for further examination and the applicant remained in custody.\\n\\nOn 7 May 2004 State Security Courts were abolished following a constitutional amendment and the applicant's case was transmitted to the Istanbul Assize Court. In the course of the proceedings, the domestic courts rejected the applicant's requests for release, taking into account the nature of the alleged offence and the documents in the case file. The applicant challenged these decisions under Article 298 of the Criminal Procedure Code; however, the domestic courts rejected all his requests.\\n\\nOn 31 January 2005 the Istanbul Assize Court found the applicant guilty and sentenced him to life imprisonment under Article 146 § 1 of the Criminal Code.\\n\\nOn 20 March 2006 the Court of Cassation quashed the decision of the Assize Court and the case file was remitted to Istanbul Assize Court.\\n\\nOn 4 October 2006 the applicant was released pending trial. According to the information in the case file, as submitted by the parties, the case is still pending before the Istanbul Assize Court.\"\n",
    "        tok_tensor = tokenizer(doc_text, return_tensors=\"tf\", padding=True, return_offsets_mapping=True)\n",
    "        tokens = tok_tensor[\"input_ids\"].numpy()[0]\n",
    "        offsets = tok_tensor[\"offset_mapping\"].numpy()[0]\n",
    "        spans_to_mask = [(2920, 2935), (343, 354), (3010, 3027), (2218, 2236), (254, 269), (1561, 1569), (516, 531), (2422, 2432), (803, 807), (2073, 2085), (841, 861), (1100, 1112), (292, 307), (1212, 1228), (1100, 1206), (3215, 3229), (1468, 1480), (1561, 1586), (3076, 3089), (379, 387), (867, 882), (1118, 1205), (2202, 2213), (3094, 3112), (2179, 2196), (54, 62)]\n",
    "    \n",
    "        labels = label_tokens(tokens, offsets, spans_to_mask)\n",
    "        expected_labels = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "        self.assertTrue((labels==expected_labels).all())\n",
    "\n",
    "  \n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f4be9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'PR', 'OC', 'ED', 'URE', '\\n', '\\n', 'The', 'case', 'originated', 'in', 'an', 'application', '(', 'no', '.', '*40*', '*593*', '*/*', '*04*', ')', 'against', 'the', 'Republic', 'of', 'Turkey', 'lodged', 'with', 'the', 'Court', 'under', 'Article', '34', 'of', 'the', 'Convention', 'for', 'the', 'Protection', 'of', 'Human', 'Rights', 'and', 'Fundamental', 'Freed', 'oms', '(', '“', '“', 'the', 'Convention', '”', '”', ')', 'by', 'a', 'Turkish', 'national', ',', '*Mr*', '*C*', '*eng*', '*iz*', '*Pol*', '*at*', '(', '“', '“', 'the', 'applicant', '”', '”', '),', 'on', '*15*', '*October*', '*2004*', '.', '\\n', '\\n', 'The', 'applicant', 'was', 'represented', 'by', '*Mr*', '*E*', '*.*', '*Kan*', '*ar*', ',', 'a', 'lawyer', 'pract', 'ising', 'in', '*Istanbul*', '.', 'The', 'Turkish', 'Government', '(', '“', '“', 'the', 'Government', '”', '”', ')', 'did', 'not', 'designate', 'an', 'Agent', 'for', 'the', 'purposes', 'of', 'the', 'proceedings', 'before', 'the', 'Court', '.', '\\n', '\\n', 'On', '*6*', '*November*', '*2006*', 'the', 'Court', 'decided', 'to', 'give', 'notice', 'of', 'the', 'application', 'to', 'the', 'Government', '.', 'App', 'lying', 'Article', '29', '§', '3', 'of', 'the', 'Convention', ',', 'it', 'decided', 'to', 'rule', 'on', 'the', 'ad', 'miss', 'ibility', 'and', 'merits', 'of', 'the', 'application', 'at', 'the', 'same', 'time', '.', '\\n', '\\n', 'THE', 'FAC', 'TS', '\\n', '\\n', 'THE', 'C', 'IRC', 'UM', 'ST', 'ANC', 'ES', 'OF', 'THE', 'CASE', '\\n', '\\n', 'The', 'applicant', 'was', 'born', 'in', '*1965*', '.', 'He', 'is', 'currently', 'detained', 'in', 'the', '*Ed*', '*ir*', '*ne*', '*F*', '*-*', '*type*', '*Prison*', '.', '\\n', '\\n', 'On', '*6*', '*February*', '*1993*', 'the', 'applicant', 'was', 'arrested', 'and', 'placed', 'in', 'police', 'custody', 'by', 'officers', 'from', 'the', 'Anti', '-', 'terror', 'branch', 'of', 'the', 'Istanbul', 'Security', 'Directorate', 'on', 'suspicion', 'of', 'involvement', 'in', 'the', 'activities', 'of', 'an', 'illegal', 'armed', 'organisation', ',', 'the', '*T*', '*K*', '*P*', '*/*', '*ML*', '*-*', '*TI*', '*KK*', '*O*', '*(*', '*the*', '*Turkish*', '*Communist*', '*Party*', '*/*', '*Marx*', '*ist*', '*Lenin*', '*ist*', '*-*', '*Turkish*', '*Workers*', '*and*', '*Pe*', '*as*', '*ants*', \"*'*\", '*Liberation*', '*Army*', ').', '\\n', '\\n', 'On', '*15*', '*February*', '*1993*', 'the', 'applicant', 'was', 'brought', 'before', 'the', 'public', 'prosecutor', 'and', 'then', 'the', 'investigating', 'judge', 'at', 'the', 'Istanbul', 'State', 'Security', 'Court', '.', 'On', 'the', 'same', 'day', 'the', 'investigating', 'judge', 'rem', 'anded', 'the', 'applicant', 'in', 'custody', 'pending', 'trial', '.', '\\n', '\\n', 'By', 'an', 'indictment', 'dated', '*5*', '*April*', '*1993*', ',', 'the', 'public', 'prosecutor', 'initiated', 'criminal', 'proceedings', 'against', 'the', 'applicant', 'and', '*nineteen*', '*other*', '*defendants*', 'before', 'the', 'Istanbul', 'State', 'Security', 'Court', ',', 'accusing', 'them', ',', 'inter', 'al', 'ia', ',', 'of', 'membership', 'of', 'an', 'illegal', 'armed', 'organisation', 'and', 'of', 'involvement', 'in', 'activities', 'which', 'undermined', 'the', 'constitutional', 'order', 'of', 'the', 'State', '.', 'The', 'prosecution', 'sought', 'the', 'death', 'penalty', 'under', 'Article', '146', '§', '1', 'of', 'the', 'Criminal', 'Code', '.', '\\n', '\\n', 'In', 'the', 'course', 'of', 'the', 'proceedings', ',', 'the', 'State', 'Security', 'Court', 'rejected', 'the', 'applicant', \"'s\", 'requests', 'for', 'release', ',', 'taking', 'into', 'account', 'the', 'nature', 'of', 'the', 'alleged', 'offence', 'and', 'the', 'state', 'of', 'the', 'evidence', '.', '\\n', '\\n', 'On', '*12*', '*June*', '*2000*', 'the', 'applicant', 'was', 'convicted', 'as', 'charged', 'by', 'the', 'Istanbul', 'State', 'Security', 'Court', 'and', 'sentenced', 'to', '*life*', '*imprisonment*', '.', '\\n', '\\n', 'On', '*15*', '*May*', '*2001*', 'the', '*Court*', '*of*', '*Cass*', '*ation*', 'qu', 'ashed', 'the', 'applicant', \"'s\", 'conviction', 'for', 'procedural', 'reasons', '.', 'The', 'case', 'was', 'rem', 'itted', 'to', 'the', 'Istanbul', 'State', 'Security', 'Court', 'for', 'further', 'examination', 'and', 'the', 'applicant', 'remained', 'in', 'custody', '.', '\\n', '\\n', 'On', '*7*', '*May*', '*2004*', 'State', 'Security', 'Courts', 'were', 'abolished', 'following', 'a', 'constitutional', 'amendment', 'and', 'the', 'applicant', \"'s\", 'case', 'was', 'transmitted', 'to', 'the', 'Istanbul', 'Ass', 'ize', 'Court', '.', 'In', 'the', 'course', 'of', 'the', 'proceedings', ',', 'the', 'domestic', 'courts', 'rejected', 'the', 'applicant', \"'s\", 'requests', 'for', 'release', ',', 'taking', 'into', 'account', 'the', 'nature', 'of', 'the', 'alleged', 'offence', 'and', 'the', 'documents', 'in', 'the', 'case', 'file', '.', 'The', 'applicant', 'challenged', 'these', 'decisions', 'under', 'Article', '298', 'of', 'the', 'Criminal', 'Procedure', 'Code', ';', 'however', ',', 'the', 'domestic', 'courts', 'rejected', 'all', 'his', 'requests', '.', '\\n', '\\n', 'On', '*31*', '*January*', '*2005*', 'the', 'Istanbul', 'Ass', 'ize', 'Court', 'found', 'the', 'applicant', 'guilty', 'and', 'sentenced', 'him', 'to', '*life*', '*imprisonment*', 'under', 'Article', '146', '§', '1', 'of', 'the', 'Criminal', 'Code', '.', '\\n', '\\n', 'On', '*20*', '*March*', '*2006*', 'the', '*Court*', '*of*', '*Cass*', '*ation*', 'qu', 'ashed', 'the', 'decision', 'of', 'the', 'Ass', 'ize', 'Court', 'and', 'the', 'case', 'file', 'was', 'rem', 'itted', 'to', 'Istanbul', 'Ass', 'ize', 'Court', '.', '\\n', '\\n', 'On', '*4*', '*October*', '*2006*', 'the', 'applicant', 'was', 'released', 'pending', 'trial', '.', 'According', 'to', 'the', 'information', 'in', 'the', 'case', 'file', ',', 'as', 'submitted', 'by', 'the', 'parties', ',', 'the', 'case', 'is', 'still', 'pending', 'before', 'the', 'Istanbul', 'Ass', 'ize', 'Court', '.', '']\n",
      "PROCEDURE\n",
      "\n",
      "Thecaseoriginatedinanapplication(no.*40**593**/**04*)againsttheRepublicofTurkeylodgedwiththeCourtunderArticle34oftheConventionfortheProtectionofHumanRightsandFundamentalFreedoms(““theConvention””)byaTurkishnational,*Mr**C**eng**iz**Pol**at*(““theapplicant””),on*15**October**2004*.\n",
      "\n",
      "Theapplicantwasrepresentedby*Mr**E**.**Kan**ar*,alawyerpractisingin*Istanbul*.TheTurkishGovernment(““theGovernment””)didnotdesignateanAgentforthepurposesoftheproceedingsbeforetheCourt.\n",
      "\n",
      "On*6**November**2006*theCourtdecidedtogivenoticeoftheapplicationtotheGovernment.ApplyingArticle29§3oftheConvention,itdecidedtoruleontheadmissibilityandmeritsoftheapplicationatthesametime.\n",
      "\n",
      "THEFACTS\n",
      "\n",
      "THECIRCUMSTANCESOFTHECASE\n",
      "\n",
      "Theapplicantwasbornin*1965*.Heiscurrentlydetainedinthe*Ed**ir**ne**F**-**type**Prison*.\n",
      "\n",
      "On*6**February**1993*theapplicantwasarrestedandplacedinpolicecustodybyofficersfromtheAnti-terrorbranchoftheIstanbulSecurityDirectorateonsuspicionofinvolvementintheactivitiesofanillegalarmedorganisation,the*T**K**P**/**ML**-**TI**KK**O**(**the**Turkish**Communist**Party**/**Marx**ist**Lenin**ist**-**Turkish**Workers**and**Pe**as**ants**'**Liberation**Army*).\n",
      "\n",
      "On*15**February**1993*theapplicantwasbroughtbeforethepublicprosecutorandthentheinvestigatingjudgeattheIstanbulStateSecurityCourt.Onthesamedaytheinvestigatingjudgeremandedtheapplicantincustodypendingtrial.\n",
      "\n",
      "Byanindictmentdated*5**April**1993*,thepublicprosecutorinitiatedcriminalproceedingsagainsttheapplicantand*nineteen**other**defendants*beforetheIstanbulStateSecurityCourt,accusingthem,interalia,ofmembershipofanillegalarmedorganisationandofinvolvementinactivitieswhichunderminedtheconstitutionalorderoftheState.TheprosecutionsoughtthedeathpenaltyunderArticle146§1oftheCriminalCode.\n",
      "\n",
      "Inthecourseoftheproceedings,theStateSecurityCourtrejectedtheapplicant'srequestsforrelease,takingintoaccountthenatureoftheallegedoffenceandthestateoftheevidence.\n",
      "\n",
      "On*12**June**2000*theapplicantwasconvictedaschargedbytheIstanbulStateSecurityCourtandsentencedto*life**imprisonment*.\n",
      "\n",
      "On*15**May**2001*the*Court**of**Cass**ation*quashedtheapplicant'sconvictionforproceduralreasons.ThecasewasremittedtotheIstanbulStateSecurityCourtforfurtherexaminationandtheapplicantremainedincustody.\n",
      "\n",
      "On*7**May**2004*StateSecurityCourtswereabolishedfollowingaconstitutionalamendmentandtheapplicant'scasewastransmittedtotheIstanbulAssizeCourt.Inthecourseoftheproceedings,thedomesticcourtsrejectedtheapplicant'srequestsforrelease,takingintoaccountthenatureoftheallegedoffenceandthedocumentsinthecasefile.TheapplicantchallengedthesedecisionsunderArticle298oftheCriminalProcedureCode;however,thedomesticcourtsrejectedallhisrequests.\n",
      "\n",
      "On*31**January**2005*theIstanbulAssizeCourtfoundtheapplicantguiltyandsentencedhimto*life**imprisonment*underArticle146§1oftheCriminalCode.\n",
      "\n",
      "On*20**March**2006*the*Court**of**Cass**ation*quashedthedecisionoftheAssizeCourtandthecasefilewasremittedtoIstanbulAssizeCourt.\n",
      "\n",
      "On*4**October**2006*theapplicantwasreleasedpendingtrial.Accordingtotheinformationinthecasefile,assubmittedbytheparties,thecaseisstillpendingbeforetheIstanbulAssizeCourt.\n"
     ]
    }
   ],
   "source": [
    "def test_masked_tokens():\n",
    "    doc_text = \"PROCEDURE\\n\\nThe case originated in an application (no. 40593/04) against the Republic of Turkey lodged with the Court under Article 34 of the Convention for the Protection of Human Rights and Fundamental Freedoms (“the Convention”) by a Turkish national, Mr Cengiz Polat (“the applicant”), on 15 October 2004.\\n\\nThe applicant was represented by Mr E. Kanar, a lawyer practising in Istanbul. The Turkish Government (“the Government”) did not designate an Agent for the purposes of the proceedings before the Court.\\n\\nOn 6 November 2006 the Court decided to give notice of the application to the Government. Applying Article 29 § 3 of the Convention, it decided to rule on the admissibility and merits of the application at the same time.\\n\\nTHE FACTS\\n\\nTHE CIRCUMSTANCES OF THE CASE\\n\\nThe applicant was born in 1965. He is currently detained in the Edirne F-type Prison.\\n\\nOn 6 February 1993 the applicant was arrested and placed in police custody by officers from the Anti-terror branch of the Istanbul Security Directorate on suspicion of involvement in the activities of an illegal armed organisation, the TKP/ML-TIKKO (the Turkish Communist Party/Marxist Leninist -Turkish Workers and Peasants' Liberation Army).\\n\\nOn 15 February 1993 the applicant was brought before the public prosecutor and then the investigating judge at the Istanbul State Security Court. On the same day the investigating judge remanded the applicant in custody pending trial.\\n\\nBy an indictment dated 5 April 1993, the public prosecutor initiated criminal proceedings against the applicant and nineteen other defendants before the Istanbul State Security Court, accusing them, inter alia, of membership of an illegal armed organisation and of involvement in activities which undermined the constitutional order of the State. The prosecution sought the death penalty under Article 146 § 1 of the Criminal Code.\\n\\nIn the course of the proceedings, the State Security Court rejected the applicant's requests for release, taking into account the nature of the alleged offence and the state of the evidence.\\n\\nOn 12 June 2000 the applicant was convicted as charged by the Istanbul State Security Court and sentenced to life imprisonment.\\n\\nOn 15 May 2001 the Court of Cassation quashed the applicant's conviction for procedural reasons. The case was remitted to the Istanbul State Security Court for further examination and the applicant remained in custody.\\n\\nOn 7 May 2004 State Security Courts were abolished following a constitutional amendment and the applicant's case was transmitted to the Istanbul Assize Court. In the course of the proceedings, the domestic courts rejected the applicant's requests for release, taking into account the nature of the alleged offence and the documents in the case file. The applicant challenged these decisions under Article 298 of the Criminal Procedure Code; however, the domestic courts rejected all his requests.\\n\\nOn 31 January 2005 the Istanbul Assize Court found the applicant guilty and sentenced him to life imprisonment under Article 146 § 1 of the Criminal Code.\\n\\nOn 20 March 2006 the Court of Cassation quashed the decision of the Assize Court and the case file was remitted to Istanbul Assize Court.\\n\\nOn 4 October 2006 the applicant was released pending trial. According to the information in the case file, as submitted by the parties, the case is still pending before the Istanbul Assize Court.\"\n",
    "    tok_tensor = tokenizer(doc_text, return_tensors=\"tf\", padding=True, return_offsets_mapping=True)\n",
    "    tokens = tok_tensor[\"input_ids\"].numpy()[0]\n",
    "    offsets = tok_tensor[\"offset_mapping\"].numpy()[0]\n",
    "    spans_to_mask = [(2920, 2935), (343, 354), (3010, 3027), (2218, 2236), (254, 269), (1561, 1569), (516, 531), (2422, 2432), (803, 807), (2073, 2085), (841, 861), (1100, 1112), (292, 307), (1212, 1228), (1100, 1206), (3215, 3229), (1468, 1480), (1561, 1586), (3076, 3089), (379, 387), (867, 882), (1118, 1205), (2202, 2213), (3094, 3112), (2179, 2196), (54, 62)]\n",
    "    \n",
    "    labels = label_tokens(tokens, offsets, spans_to_mask)\n",
    "    \n",
    "    masked_doc_text = []\n",
    "    for token, offset, label in zip(tokens, offsets, labels):\n",
    "        if label == 1:\n",
    "            #masked_doc_text.append(\"[MASK]\")\n",
    "            str=\"*\" + doc_text[offset[0]:offset[1]] +\"*\"        \n",
    "            masked_doc_text.append(str)\n",
    "        else:\n",
    "            masked_doc_text.append(doc_text[offset[0]:offset[1]])\n",
    "    print(masked_doc_text)\n",
    "    masked_doc_text_joined = ''.join(x for x in masked_doc_text)\n",
    "    print(masked_doc_text_joined)\n",
    "\n",
    "test_masked_tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6947002",
   "metadata": {},
   "source": [
    "## Create Labels and Tokenize Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8411938a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create labels\n",
    "\n",
    "dev_text = []\n",
    "dev_labels = []\n",
    "\n",
    "for i in range(len(dev_file)):\n",
    "    doc_id = dev_file[i][\"doc_id\"]\n",
    "    spans_to_mask = dev_masks[doc_id]\n",
    "    spans_to_mask = list({tuple(x) for x in spans_to_mask}) # Make spans unique\n",
    "    doc_text = dev_file[i][\"text\"]\n",
    "    tok_tensor = tokenizer(doc_text, return_tensors=\"tf\", truncation=True, padding=True, return_offsets_mapping=True)\n",
    "    tokens = tok_tensor[\"input_ids\"].numpy()[0]\n",
    "    offsets = tok_tensor[\"offset_mapping\"].numpy()[0]\n",
    "    dev_text.append(doc_text)\n",
    "    dev_labels.append(label_tokens(tokens, offsets, spans_to_mask))\n",
    "    \n",
    "train_text = []\n",
    "train_labels = []\n",
    "\n",
    "for i in range(len(train_file)):\n",
    "    doc_id = train_file[i][\"doc_id\"]\n",
    "    spans_to_mask = train_masks[doc_id]\n",
    "    spans_to_mask = list({tuple(x) for x in spans_to_mask}) # Make spans unique\n",
    "    doc_text = train_file[i][\"text\"]\n",
    "    tok_tensor = tokenizer(doc_text, return_tensors=\"tf\", truncation=True, padding=True, return_offsets_mapping=True)\n",
    "    tokens = tok_tensor[\"input_ids\"].numpy()[0]\n",
    "    offsets = tok_tensor[\"offset_mapping\"].numpy()[0]\n",
    "    \n",
    "    train_text.append(doc_text)\n",
    "    train_labels.append(label_tokens(tokens, offsets, spans_to_mask))\n",
    "\n",
    "test_text = []\n",
    "test_labels = []\n",
    "\n",
    "for i in range(len(test_file)):\n",
    "    doc_id = test_file[i][\"doc_id\"]\n",
    "    spans_to_mask = test_masks[doc_id]\n",
    "    spans_to_mask = list({tuple(x) for x in spans_to_mask}) # Make spans unique\n",
    "    doc_text = test_file[i][\"text\"]\n",
    "    tok_tensor = tokenizer(doc_text, return_tensors=\"tf\", truncation=True, padding=True, return_offsets_mapping=True)\n",
    "    tokens = tok_tensor[\"input_ids\"].numpy()[0]\n",
    "    offsets = tok_tensor[\"offset_mapping\"].numpy()[0]\n",
    "    \n",
    "    test_text.append(doc_text)\n",
    "    test_labels.append(label_tokens(tokens, offsets, spans_to_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b7c850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad labels to max length\n",
    "\n",
    "MAX_LEN = 4096\n",
    "\n",
    "for i in range(len(dev_labels)):\n",
    "    curr_len = len(dev_labels[i])\n",
    "    \n",
    "    if curr_len < MAX_LEN:\n",
    "        to_add = [0] * (MAX_LEN - curr_len)\n",
    "        dev_labels[i].extend(to_add)\n",
    "        \n",
    "for i in range(len(train_labels)):\n",
    "    curr_len = len(train_labels[i])\n",
    "    \n",
    "    if curr_len < MAX_LEN:\n",
    "        to_add = [0] * (MAX_LEN - curr_len)\n",
    "        train_labels[i].extend(to_add)\n",
    "        \n",
    "for i in range(len(test_labels)):\n",
    "    curr_len = len(test_labels[i])\n",
    "    \n",
    "    if curr_len < 2594:  # Max sequence length in test set\n",
    "        to_add = [0] * (2594 - curr_len)\n",
    "        test_labels[i].extend(to_add)\n",
    "        \n",
    "dev_labels = np.asarray(dev_labels)\n",
    "train_labels = np.asarray(train_labels)\n",
    "test_labels = np.asarray(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ebaee469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize input\n",
    "\n",
    "dev_text_tokenized = tokenizer(dev_text, truncation=True, padding=True, return_tensors=\"tf\")\n",
    "train_text_tokenized = tokenizer(train_text, truncation=True, padding=True, return_tensors=\"tf\")\n",
    "test_text_tokenized = tokenizer(test_text, truncation=True, padding=True, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001d3735",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe838a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00002),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e0fc226",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-03 00:21:28.570963: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: tf_longformer_for_token_classification/longformer/encoder/layer_._9/attention/self/cond_2/branch_executed/_1009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1014/1014 [==============================] - 13355s 13s/step - loss: 0.0486 - accuracy: 0.9817\n",
      "Epoch 2/2\n",
      "1014/1014 [==============================] - 13218s 13s/step - loss: 0.0346 - accuracy: 0.9863\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_text_tokenized[\"input_ids\"],\n",
    "                    train_labels,\n",
    "                    batch_size=1,\n",
    "                    epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb0581b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "\n",
    "model.save_pretrained(\"../models/longformer_baseline.h5\", save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8de3bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [0.04859026148915291, 0.03456227853894234], 'accuracy': [0.9817197918891907, 0.9862558841705322]}\n"
     ]
    }
   ],
   "source": [
    "print(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae03832d",
   "metadata": {},
   "source": [
    "# Assess Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9674d387",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ../models/longformer_baseline.h5 were not used when initializing TFLongformerForTokenClassification: ['dropout_49']\n",
      "- This IS expected if you are initializing TFLongformerForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFLongformerForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFLongformerForTokenClassification were initialized from the model checkpoint at ../models/longformer_baseline.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFLongformerForTokenClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model\n",
    "\n",
    "model = TFLongformerForTokenClassification.from_pretrained(\"../models/longformer_baseline.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88317ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_longformer_for_token_classification_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " longformer (TFLongformerMai  multiple                 148068864 \n",
      " nLayer)                                                         \n",
      "                                                                 \n",
      " dropout_99 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 148,070,402\n",
      "Trainable params: 148,070,402\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3560358",
   "metadata": {},
   "source": [
    "## Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "78458baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(test_text_tokenized[\"input_ids\"], labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d5f28244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.07335881], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print (preds.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a1628103",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = preds.logits\n",
    "predicted_token_class_ids = tf.math.argmax(logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40fbe307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(127, 2594), dtype=int64, numpy=\n",
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_token_class_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "323d4e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predicted_token_class_ids to avoid running inference again\n",
    "\n",
    "np.savetxt(\"../predictions/longformer_preds_tab_test_set.txt\", predicted_token_class_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9ab100",
   "metadata": {},
   "source": [
    "## Wikipedia\n",
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aec65e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load wiki data\n",
    "\n",
    "with open(\"../data/raw/wiki-summaries/annotated_wikipedia.json\") as file:\n",
    "    wiki_file = json.load(file)\n",
    "\n",
    "with open(\"../data/processed/wiki_masks.json\") as file:\n",
    "    wiki_masks = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3828345b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels\n",
    "\n",
    "wiki_text = []\n",
    "wiki_labels = []\n",
    "\n",
    "for i in range(len(wiki_file)):\n",
    "    doc_id = wiki_file[i][\"doc_id\"]\n",
    "    spans_to_mask = wiki_masks[doc_id]\n",
    "    spans_to_mask = list({tuple(x) for x in spans_to_mask}) # Make spans unique\n",
    "    doc_text = wiki_file[i][\"text\"]\n",
    "    tok_tensor = tokenizer(doc_text, return_tensors=\"tf\", truncation=True, padding=True, return_offsets_mapping=True)\n",
    "    tokens = tok_tensor[\"input_ids\"].numpy()[0]\n",
    "    offsets = tok_tensor[\"offset_mapping\"].numpy()[0]\n",
    "    wiki_text.append(doc_text)\n",
    "    wiki_labels.append(label_tokens(tokens, offsets, spans_to_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "341642d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad labels to max length\n",
    "\n",
    "MAX_LEN = 730 # Longest sequence in wiki dataset\n",
    "\n",
    "for i in range(len(wiki_labels)):\n",
    "    curr_len = len(wiki_labels[i])\n",
    "    \n",
    "    if curr_len < MAX_LEN:\n",
    "        to_add = [0] * (MAX_LEN - curr_len)\n",
    "        wiki_labels[i].extend(to_add)\n",
    "        \n",
    "wiki_labels = np.asarray(wiki_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aaa71eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize input\n",
    "\n",
    "wiki_text_tokenized = tokenizer(wiki_text, truncation=True, padding=True, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acf3e2a",
   "metadata": {},
   "source": [
    "### Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8aaf4717",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(wiki_text_tokenized[\"input_ids\"], labels=wiki_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "361eaa20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.05058768], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print (preds.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c4ea9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = preds.logits\n",
    "predicted_token_class_ids = tf.math.argmax(logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8a31892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(553, 730), dtype=int64, numpy=\n",
       "array([[0, 1, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]])>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_token_class_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "737f4964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predicted_token_class_ids to avoid running inference again\n",
    "\n",
    "np.savetxt(\"../predictions/longformer_preds.txt\", predicted_token_class_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf822d62",
   "metadata": {},
   "source": [
    "## Calculate Precision, Recall, and AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "40a78048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_precision(pred_list, label_list):\n",
    "    \"\"\"Calculates precision of batch of predictions\"\"\"\n",
    "    \n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    \n",
    "    for i in range(len(pred_list)):\n",
    "        for j in range(len(pred_list[i])):\n",
    "        \n",
    "            if pred_list[i][j] == 1:\n",
    "                if label_list[i][j] == 1:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    fp += 1\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "    return tp / (tp + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2b4b5d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_recall(pred_list, label_list):\n",
    "    \"\"\"Calculates recall of batch of predictions\"\"\"\n",
    "    \n",
    "    tp = 0 \n",
    "    fn = 0\n",
    "    \n",
    "    for i in range(len(pred_list)):\n",
    "        for j in range(len(pred_list[i])):\n",
    "            \n",
    "            if pred_list[i][j] == 1:\n",
    "                if label_list[i][j] == 1:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    tp += 0\n",
    "                \n",
    "            else:\n",
    "                if label_list[i][j] == 1:\n",
    "                    fn += 1\n",
    "                else:\n",
    "                    fn += 0\n",
    "    \n",
    "    return tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5518a73e",
   "metadata": {},
   "source": [
    "## Test Set - Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f9657d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Token level precision: 0.9657204374185506\n"
     ]
    }
   ],
   "source": [
    "precision = calc_precision(predicted_token_class_ids, test_labels)\n",
    "print (f' Token level precision: {precision}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ffb602c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Token level recall: 0.7202806068545831\n"
     ]
    }
   ],
   "source": [
    "recall = calc_recall(predicted_token_class_ids, test_labels)\n",
    "print (f' Token level recall: {recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a0e417ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Average AUC: 0.8745893287470833\n"
     ]
    }
   ],
   "source": [
    "# Multilabel indicators are not supported in sklearn for AUC\n",
    "# Loop through preds and take avg of AUC\n",
    "\n",
    "auc = []\n",
    "\n",
    "for i in range(len(predicted_token_class_ids)):\n",
    "    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(test_labels[i], predicted_token_class_ids[i], pos_label=1)\n",
    "    auc.append(metrics.auc(fpr, tpr))\n",
    "\n",
    "auc = sum(auc)/len(auc)\n",
    "print (f' Average AUC: {auc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978bdd02",
   "metadata": {},
   "source": [
    "## Wikipedia - Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b33b7048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Token level precision: 0.9302261898241962\n"
     ]
    }
   ],
   "source": [
    "precision = calc_precision(predicted_token_class_ids, wiki_labels)\n",
    "print (f' Token level precision: {precision}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "25b4dad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Token level recall: 0.6816631215635911\n"
     ]
    }
   ],
   "source": [
    "recall = calc_recall(predicted_token_class_ids, wiki_labels)\n",
    "print (f' Token level recall: {recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7e861731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Average AUC: 0.8524968985862243\n"
     ]
    }
   ],
   "source": [
    "# Multilabel indicators are not supported in sklearn for AUC\n",
    "# Loop through preds and take avg of AUC\n",
    "\n",
    "auc = []\n",
    "\n",
    "for i in range(len(predicted_token_class_ids)):\n",
    "    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(wiki_labels[i], predicted_token_class_ids[i], pos_label=1)\n",
    "    auc.append(metrics.auc(fpr, tpr))\n",
    "\n",
    "auc = sum(auc)/len(auc)\n",
    "print (f' Average AUC: {auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6fed26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "80c80847a93213b192f70e81653c0285be2eeca283682b4031421ab3d792b00b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
