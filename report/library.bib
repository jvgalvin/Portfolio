%Process to pull .bib entries: search Berkeley library, at the bottom of page select Export .bib -> UTF-8 
@book{UN,
  author = {{United Nations}},
  title = {Universal Declaration of Human Rights},
  year = {1948},
  month = dec,
  venue = {Paris},
  month_numeric = {12}
}
@book{10.5555/3152676,
author = {Voigt, Paul and Bussche, Axel von dem},
title = {The EU General Data Protection Regulation (GDPR): A Practical Guide},
year = {2017},
isbn = {3319579584},
publisher = {Springer Publishing Company, Incorporated},
edition = {1st},
abstract = {This book provides expert advice on the practical implementation of the European Unions General Data Protection Regulation (GDPR) and systematically analyses its various provisions. Examples, tables, a checklist etc. showcase the practical consequences of the new legislation. The handbook examines the GDPRs scope of application, the organizational and material requirements for data protection, the rights of data subjects, the role of the Supervisory Authorities, enforcement and fines under the GDPR, and national particularities. In addition, it supplies a brief outlook on the legal consequences for seminal data processing areas, such as Cloud Computing, Big Data and the Internet of Things. Adopted in 2016, the General Data Protection Regulation will come into force in May 2018. It provides for numerous new and intensified data protection obligations, as well as a significant increase in fines (up to 20 million euros). As a result, not only companies located within the European Union will have to change their approach to data security; due to the GDPRs broad, transnational scope of application, it will affect numerous companies worldwide.}
}
@book{10.2307/j.ctvjghvnn,
 ISBN = {9781787781320},
 URL = {http://www.jstor.org/stable/j.ctvjghvnn},
 abstract = { Understand the CCPA (California Consumer Privacy Act) and how to implement strategies to comply with this privacy regulation. Established in June 2018, the CCPA was created to remedy the lack of comprehensive privacy regulation in the state of California. When it comes into effect on January 1, 2020, the CCPA will give California residents the right to:   Learn what personal data a business has collected about them Understand who this data has been disclosed to Find out whether their personal data has been sold to third parties, and who these third parties are Opt out of such data transactions, or request that the data be deleted.  Many organizations that do business in the state of California must align to the provisions of the CCPA. Much like the EU's GDPR (General Data Protection Regulation), businesses that fail to comply with the CCPA will face economic penalties.  Prepare your business for CCPA compliance with our implementation guide that:   Provides the reader with a comprehensive understanding of the legislation by explaining key terms Explains how a business can implement strategies to comply with the CCPA Discusses potential developments of the CCPA to further aid compliance  Your guide to understanding the CCPA and how you can implement a strategy to comply with this legislation - buy this book today to get the guidance you need!  About the author  Preston Bukaty is an attorney and consultant. He specializes in data privacy GRC projects, from data inventory audits to gap analyses, contract management, and remediation planning. His compliance background and experience operationalizing compliance in a variety of industries give him a strong understanding of the legal issues presented by international regulatory frameworks. Having conducted more than 3,000 data mapping audits, he also understands the practical realities of project management in operationalizing compliance initiatives.   Preston's legal experience and enthusiasm for technology make him uniquely suited to understanding the business impact of privacy regulations such as the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA). He has advised more than 250 organizations engaged in businesses as varied as SaaS platforms, mobile geolocation applications, GNSS/telematics tools, financial institutions, fleet management software, architectural/engineering design systems, and web hosting. He also teaches certification courses on GDPR compliance and ISO 27001 implementation, and writes on data privacy law topics.   Preston lives in Denver, Colorado. Prior to working as a data privacy consultant, he worked for an international GPS software company, advising business areas on compliance issues across 140 countries. Preston holds a juris doctorate from the University of Kansas School of Law, along with a basketball signed by Hall of Fame coach Bill Self.   },
 author = {PRESTON BUKATY},
 publisher = {IT Governance Publishing},
 title = {The California Consumer Privacy Act (CCPA): An implementation guide},
 urldate = {2022-12-03},
 year = {2019}
}
@article{NingYichen2021DLbP,
abstract = {Abstract
Data sharing sometimes brings the privacy disclosure risk. Anonymization methods such as k-anonymity, l-diversity prevent privacy disclosure, but such methods are suitable for structured text. There are a lot of unstructured texts in people’s lives (such as social network texts, clinical texts), and identifying and structuring the private information(PI) of unstructured texts is a problem. Based on this, we propose a deep learning-based unstructured text PI identification approach, which can extract PI in unstructured text, associate the PI with the corresponding subject, and organize it into structured data, to support follow-up anonymization. This approach is divided into two tasks: PI identification and PI association. we respectively propose a sequence labeling model based on the RoBERTa-BiLSTM-CRF hybrid neural network and a PI association method based on the RoBERTa-HCR hybrid neural network to identify PI and organize it into structured data. The experimental results show that, compared with the benchmark model, RoBEERTa-BiLSTM-CRF has better performance; compared with the current Chinese coreference resolution model, the average F1-score value of RoBERTa-HCR is increased by 6%.},
author = {Ning, Yichen and Wang, Na and Liu, Aodi and du, Xuehui},
address = {Bristol},
copyright = {2021. This work is published under http://creativecommons.org/licenses/by/3.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
issn = {1742-6588},
journal = {Journal of physics. Conference series},
keywords = {Data retrieval ; Deep learning ; Neural networks ; Privacy ; Social networks ; Texts ; Unstructured data},
language = {eng},
number = {1},
pages = {12032-},
publisher = {IOP Publishing},
title = {Deep Learning based Privacy Information Identification approach for Unstructured Text},
volume = {1848},
year = {2021},
}
@article{BeltagyLongformer,
  author    = {Iz Beltagy and
               Matthew E. Peters and
               Arman Cohan},
  title     = {Longformer: The Long-Document Transformer},
  journal   = {CoRR},
  volume    = {abs/2004.05150},
  year      = {2020},
  url       = {https://arxiv.org/abs/2004.05150},
  eprinttype = {arXiv},
  eprint    = {2004.05150},
  timestamp = {Tue, 14 Apr 2020 16:40:34 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-05150.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{CohenSPECTER,
  author    = {Arman Cohan and
               Sergey Feldman and
               Iz Beltagy and
               Doug Downey and
               Daniel S. Weld},
  title     = {{SPECTER:} Document-level Representation Learning using Citation-informed
               Transformers},
  journal   = {CoRR},
  volume    = {abs/2004.07180},
  year      = {2020},
  url       = {https://arxiv.org/abs/2004.07180},
  eprinttype = {arXiv},
  eprint    = {2004.07180},
  timestamp = {Sat, 23 Jan 2021 01:11:17 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-07180.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{he2021debertav3,
      title={DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing}, 
      author={Pengcheng He and Jianfeng Gao and Weizhu Chen},
      year={2021},
      eprint={2111.09543},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{DevlinBERTNER,
  doi = {10.48550/ARXIV.1810.04805},
  
  url = {https://arxiv.org/abs/1810.04805},
  
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{WangSUPERGLUE,
  doi = {10.48550/ARXIV.1905.00537},
  
  url = {https://arxiv.org/abs/1905.00537},
  
  author = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@inproceedings{ZhouGD2002Neru,
abstract = {This paper proposes a Hidden Markov Model (HMM) and an HMM-based chunk tagger, from which a named entity (NE) recognition (NER) system is built to recognize and classify names, times and numerical quantities. Through the HMM, our system is able to apply and integrate four types of internal and external evidences: 1) simple deterministic internal feature of the words, such as capitalization and digitalization; 2) internal semantic feature of important triggers; 3) internal gazetteer feature; 4) external macro context feature. In this way, the NER problem can be resolved effectively. Evaluation of our system on MUC-6 and MUC-7 English NE tasks achieves F-measures of 96.6% and 94.1% respectively. It shows that the performance is significantly better than reported by any other machine-learning system. Moreover, the performance is even consistently better than those based on handcrafted rules.},
author = {Zhou, GD and Su, J},
address = {SOMERSET},
booktitle = {40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE},
isbn = {1558608834},
keywords = {Computer Science ; Computer Science, Artificial Intelligence ; Computer Science, Interdisciplinary Applications ; Linguistics ; Mathematics ; Physical Sciences ; Science & Technology ; Social Sciences ; Statistics & Probability ; Technology},
language = {eng},
organization = {ACL},
pages = {473-480},
publisher = {Assoc Computational Linguistics},
title = {Named entity recognition using an HMM-based chunk tagger},
year = {2002},
}
@article{PilánIldikó2022TTAB,
abstract = {We present a novel benchmark and associated evaluation metrics for assessing
the performance of text anonymization methods. Text anonymization, defined as
the task of editing a text document to prevent the disclosure of personal
information, currently suffers from a shortage of privacy-oriented annotated
text resources, making it difficult to properly evaluate the level of privacy
protection offered by various anonymization methods. This paper presents TAB
(Text Anonymization Benchmark), a new, open-source annotated corpus developed
to address this shortage. The corpus comprises 1,268 English-language court
cases from the European Court of Human Rights (ECHR) enriched with
comprehensive annotations about the personal information appearing in each
document, including their semantic category, identifier type, confidential
attributes, and co-reference relations. Compared to previous work, the TAB
corpus is designed to go beyond traditional de-identification (which is limited
to the detection of predefined semantic categories), and explicitly marks which
text spans ought to be masked in order to conceal the identity of the person to
be protected. Along with presenting the corpus and its annotation layers, we
also propose a set of evaluation metrics that are specifically tailored towards
measuring the performance of text anonymization, both in terms of privacy
protection and utility preservation. We illustrate the use of the benchmark and
the proposed metrics by assessing the empirical performance of several baseline
text anonymization models. The full corpus along with its privacy-oriented
annotation guidelines, evaluation scripts and baseline models are available on:
https://github.com/NorskRegnesentral/text-anonymisation-benchmark},
author = {Pilán, Ildikó and Lison, Pierre and Øvrelid, Lilja and Papadopoulou, Anthi and Sánchez, David and Batet, Montserrat},
copyright = {http://creativecommons.org/licenses/by/4.0},
language = {eng},
title = {The Text Anonymization Benchmark (TAB): A Dedicated Corpus and Evaluation Framework for Text Anonymization},
year = {2022},
}
@inproceedings{wang2019glue,
     title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
     author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
     note={In the Proceedings of ICLR.},
     year={2019}
 }
 @misc{RajpurkarSQUAD2,
  doi = {10.48550/ARXIV.1806.03822},
  
  url = {https://arxiv.org/abs/1806.03822},
  
  author = {Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Know What You Don't Know: Unanswerable Questions for SQuAD},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{HePengchengDeBerta2020,
  doi = {10.48550/ARXIV.2006.03654},
  
  url = {https://arxiv.org/abs/2006.03654},
  
  author = {He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences, I.2; I.7, cs.CL, cs.GL},
  
  title = {DeBERTa: Decoding-enhanced BERT with Disentangled Attention},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@article{ClarkKevin2020Epte,
year = {2020},
abstract = {Masked language modeling (MLM) pre-training methods such as BERT corrupt the
input by replacing some tokens with [MASK] and then train a model to
reconstruct the original tokens. While they produce good results when
transferred to downstream NLP tasks, they generally require large amounts of
compute to be effective. As an alternative, we propose a more sample-efficient
pre-training task called replaced token detection. Instead of masking the
input, our approach corrupts it by replacing some tokens with plausible
alternatives sampled from a small generator network. Then, instead of training
a model that predicts the original identities of the corrupted tokens, we train
a discriminative model that predicts whether each token in the corrupted input
was replaced by a generator sample or not. Thorough experiments demonstrate
this new pre-training task is more efficient than MLM because the task is
defined over all input tokens rather than just the small subset that was masked
out. As a result, the contextual representations learned by our approach
substantially outperform the ones learned by BERT given the same model size,
data, and compute. The gains are particularly strong for small models; for
example, we train a model on one GPU for 4 days that outperforms GPT (trained
using 30x more compute) on the GLUE natural language understanding benchmark.
Our approach also works well at scale, where it performs comparably to RoBERTa
and XLNet while using less than 1/4 of their compute and outperforms them when
using the same amount of compute.},
author = {Clark, Kevin and Luong, Minh-Thang and Le, Quoc V and Manning, Christopher D},
copyright = {http://arxiv.org/licenses/nonexclusive-distrib/1.0},
keywords = {Computer Science - Computation and Language},
language = {eng},
title = {Electra: pre-training text encoders as discriminators rather than generators},
}
@inproceedings{BeigiGhazaleh2019PPTR,
abstract = {Online users generate tremendous amounts of textual information by participating in different online activities. This data provides opportunities for researchers and business partners to understand individuals. However, this user-generated textual data not only can reveal the identity of the user but also may contain individual's private attribute information. Publishing the textual data thus compromises the privacy of users. It is challenging to design effective anonymization techniques for textual information which minimize the chances of re-identification and does not contain private information while retaining the textual semantic meaning. In this paper, we study this problem and propose a novel double privacy preserving text representation learning framework, DPText. We show the effectiveness of DPText in preserving privacy and utility.},
author = {Beigi, Ghazaleh and Shu, Kai and Guo, Ruocheng and Wang, Suhang and Liu, Huan},
address = {NEW YORK},
booktitle = {PROCEEDINGS OF THE 30TH ACM CONFERENCE ON HYPERTEXT AND SOCIAL MEDIA (HT '19)},
isbn = {1450368859},
keywords = {adversarial learning ; Computer Science ; Computer Science, Information Systems ; Computer Science, Software Engineering ; differential privacy ; Science & Technology ; Technology ; text representation ; utility},
language = {eng},
organization = {ACM},
pages = {275-276},
publisher = {ACM},
series = {HT '19},
title = {Privacy Preserving Text Representation Learning},
year = {2019},
}
@article{WeidingerLaura2021Easr,
abstract = {This paper aims to help structure the risk landscape associated with
large-scale Language Models (LMs). In order to foster advances in responsible
innovation, an in-depth understanding of the potential risks posed by these
models is needed. A wide range of established and anticipated risks are
analysed in detail, drawing on multidisciplinary expertise and literature from
computer science, linguistics, and social sciences.
We outline six specific risk areas: I. Discrimination, Exclusion and
Toxicity, II. Information Hazards, III. Misinformation Harms, V. Malicious
Uses, V. Human-Computer Interaction Harms, VI. Automation, Access, and
Environmental Harms. The first area concerns the perpetuation of stereotypes,
unfair discrimination, exclusionary norms, toxic language, and lower
performance by social group for LMs. The second focuses on risks from private
data leaks or LMs correctly inferring sensitive information. The third
addresses risks arising from poor, false or misleading information including in
sensitive domains, and knock-on risks such as the erosion of trust in shared
information. The fourth considers risks from actors who try to use LMs to cause
harm. The fifth focuses on risks specific to LLMs used to underpin
conversational agents that interact with human users, including unsafe use,
manipulation or deception. The sixth discusses the risk of environmental harm,
job automation, and other challenges that may have a disparate effect on
different social groups or communities.
In total, we review 21 risks in-depth. We discuss the points of origin of
different risks and point to potential mitigation approaches. Lastly, we
discuss organisational responsibilities in implementing mitigations, and the
role of collaboration and participation. We highlight directions for further
research, particularly on expanding the toolkit for assessing and evaluating
the outlined risks in LMs.},
author = {Weidinger, Laura and Mellor, John and Rauh, Maribeth and Griffin, Conor and Uesato, Jonathan and Huang, Po-Sen and Cheng, Myra and Glaese, Mia and Balle, Borja and Kasirzadeh, Atoosa and Kenton, Zac and Brown, Sasha and Hawkins, Will and Stepleton, Tom and Biles, Courtney and Birhane, Abeba and Haas, Julia and Rimell, Laura and Hendricks, Lisa Anne and Isaac, William and Legassick, Sean and Irving, Geoffrey and Gabriel, Iason},
copyright = {http://creativecommons.org/licenses/by/4.0},
language = {eng},
title = {Ethical and social risks of harm from Language Models},
year = {2021},
}
@article{WangACE,
  author    = {Xinyu Wang and
               Yong Jiang and
               Nguyen Bach and
               Tao Wang and
               Zhongqiang Huang and
               Fei Huang and
               Kewei Tu},
  title     = {Automated Concatenation of Embeddings for Structured Prediction},
  journal   = {CoRR},
  volume    = {abs/2010.05006},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.05006},
  eprinttype = {arXiv},
  eprint    = {2010.05006},
  timestamp = {Fri, 29 Apr 2022 09:10:29 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2010-05006.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
