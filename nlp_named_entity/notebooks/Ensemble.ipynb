{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbd8188b",
   "metadata": {},
   "source": [
    "# Ensemble\n",
    "\n",
    "Feed the final hidden states from Longformer and the final hidden states from Specter into a fully-connected feedforward network for token-level classification\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "603e4761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import LongformerTokenizerFast, AutoTokenizer, TFLongformerModel, TFAutoModel\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "from sklearn.metrics import auc, plot_precision_recall_curve, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46c86744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in data\n",
    "\n",
    "DEV_MASKS_FILE =    \"../data/processed/jg_dev_masks.json\"\n",
    "TRAIN_MASKS_FILE =  \"../data/processed/jg_train_masks.json\"\n",
    "TEST_MASKS_FILE =   \"../data/processed/jg_test_masks.json\"\n",
    "\n",
    "with open(\"../data/raw/text-anonymization-benchmark/echr_dev.json\") as file:\n",
    "    dev_file = json.load(file)\n",
    "\n",
    "with open(DEV_MASKS_FILE) as file:\n",
    "    dev_masks = json.load(file)\n",
    "    \n",
    "with open(\"../data/raw/text-anonymization-benchmark/echr_train.json\") as file:\n",
    "    train_file = json.load(file)\n",
    "    \n",
    "with open(TRAIN_MASKS_FILE) as file:\n",
    "    train_masks = json.load(file)\n",
    "       \n",
    "with open(\"../data/raw/text-anonymization-benchmark/echr_test.json\") as file:\n",
    "    test_file = json.load(file)\n",
    "\n",
    "with open(TEST_MASKS_FILE) as file:\n",
    "    test_masks = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195c8fc7",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "871c636e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to label data\n",
    "\n",
    "def label_tokens(toks, offs, spans_to_mask):\n",
    "    \"\"\"Args: \n",
    "            toks - list of token id's\n",
    "            offs - list of char offsets for each token\n",
    "       Returns:\n",
    "            label_list - 0 for non_mask, 1 for mask\"\"\"\n",
    "    \n",
    "    label_list = []\n",
    "    mapping_list = []\n",
    "    \n",
    "    # Map token_ids back to string\n",
    "    \n",
    "    for token, pos in zip(toks, offs):\n",
    "        mapping_list.append([token, pos[0], pos[1]])\n",
    "    \n",
    "    # Determine if each token should be masked\n",
    "    spans_to_mask.sort(key=lambda tup: tup[0]) #order spans, ascending\n",
    "    \n",
    "    j=0\n",
    "    \n",
    "    for i in range(len(mapping_list)):\n",
    "        \n",
    "        temp_list = []\n",
    "        stop=False\n",
    "        \n",
    "        while not stop and j < len(spans_to_mask):\n",
    "            \n",
    "            if (mapping_list[i][1] >= spans_to_mask[j][0]) and (mapping_list[i][2] <= spans_to_mask[j][1]):\n",
    "                temp_list.append(1)\n",
    "            else:\n",
    "                temp_list.append(0)           \n",
    "\n",
    "            # Since spans and mapping_list are ordered, break to allow it to catch up\n",
    "            if(spans_to_mask[j][1] > mapping_list[i][2]):\n",
    "                stop=True\n",
    "            else:\n",
    "                j = j+1\n",
    "            \n",
    "        if sum(temp_list) >= 1:\n",
    "            label_list.append(1)\n",
    "        else:\n",
    "            label_list.append(0)\n",
    "        \n",
    "    \n",
    "    return label_list  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53317ade",
   "metadata": {},
   "source": [
    "## Longformer\n",
    "### Create Labels and Tokenize Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "090afc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in tokenizer\n",
    "\n",
    "tokenizer = LongformerTokenizerFast.from_pretrained(\"allenai/longformer-base-4096\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dedf6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-25 21:23:33.391880: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Create labels\n",
    "\n",
    "dev_text = []\n",
    "dev_labels = []\n",
    "\n",
    "for i in range(len(dev_file)):\n",
    "    doc_id = dev_file[i][\"doc_id\"]\n",
    "    spans_to_mask = dev_masks[doc_id]\n",
    "    spans_to_mask = list({tuple(x) for x in spans_to_mask}) # Make spans unique\n",
    "    doc_text = dev_file[i][\"text\"]\n",
    "    tok_tensor = tokenizer(doc_text, return_tensors=\"tf\", truncation=True, padding=True, return_offsets_mapping=True)\n",
    "    tokens = tok_tensor[\"input_ids\"].numpy()[0]\n",
    "    offsets = tok_tensor[\"offset_mapping\"].numpy()[0]\n",
    "    dev_text.append(doc_text)\n",
    "    dev_labels.append(label_tokens(tokens, offsets, spans_to_mask))\n",
    "    \n",
    "train_text = []\n",
    "train_labels = []\n",
    "\n",
    "for i in range(len(train_file)):\n",
    "    doc_id = train_file[i][\"doc_id\"]\n",
    "    spans_to_mask = train_masks[doc_id]\n",
    "    spans_to_mask = list({tuple(x) for x in spans_to_mask}) # Make spans unique\n",
    "    doc_text = train_file[i][\"text\"]\n",
    "    tok_tensor = tokenizer(doc_text, return_tensors=\"tf\", truncation=True, padding=True, return_offsets_mapping=True)\n",
    "    tokens = tok_tensor[\"input_ids\"].numpy()[0]\n",
    "    offsets = tok_tensor[\"offset_mapping\"].numpy()[0]\n",
    "    \n",
    "    train_text.append(doc_text)\n",
    "    train_labels.append(label_tokens(tokens, offsets, spans_to_mask))\n",
    "\n",
    "test_text = []\n",
    "test_labels = []\n",
    "\n",
    "for i in range(len(test_file)):\n",
    "    doc_id = test_file[i][\"doc_id\"]\n",
    "    spans_to_mask = test_masks[doc_id]\n",
    "    spans_to_mask = list({tuple(x) for x in spans_to_mask}) # Make spans unique\n",
    "    doc_text = test_file[i][\"text\"]\n",
    "    tok_tensor = tokenizer(doc_text, return_tensors=\"tf\", truncation=True, padding=True, return_offsets_mapping=True)\n",
    "    tokens = tok_tensor[\"input_ids\"].numpy()[0]\n",
    "    offsets = tok_tensor[\"offset_mapping\"].numpy()[0]\n",
    "    \n",
    "    test_text.append(doc_text)\n",
    "    test_labels.append(label_tokens(tokens, offsets, spans_to_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a93ccad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad labels to max length\n",
    "\n",
    "MAX_LEN = 4096\n",
    "\n",
    "for i in range(len(dev_labels)):\n",
    "    curr_len = len(dev_labels[i])\n",
    "    \n",
    "    if curr_len < MAX_LEN:\n",
    "        to_add = [0] * (MAX_LEN - curr_len)\n",
    "        dev_labels[i].extend(to_add)\n",
    "        \n",
    "for i in range(len(train_labels)):\n",
    "    curr_len = len(train_labels[i])\n",
    "    \n",
    "    if curr_len < MAX_LEN:\n",
    "        to_add = [0] * (MAX_LEN - curr_len)\n",
    "        train_labels[i].extend(to_add)\n",
    "        \n",
    "for i in range(len(test_labels)):\n",
    "    curr_len = len(test_labels[i])\n",
    "    \n",
    "    if curr_len < MAX_LEN:\n",
    "        to_add = [0] * (MAX_LEN - curr_len)\n",
    "        test_labels[i].extend(to_add)\n",
    "        \n",
    "dev_labels = np.asarray(dev_labels)\n",
    "train_labels = np.asarray(train_labels)\n",
    "test_labels = np.asarray(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1abf82d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize input\n",
    "\n",
    "dev_text_tokenized = tokenizer(dev_text, truncation=True, padding=\"max_length\", return_tensors=\"tf\")\n",
    "train_text_tokenized = tokenizer(train_text, truncation=True, padding=\"max_length\", return_tensors=\"tf\")\n",
    "test_text_tokenized = tokenizer(test_text, truncation=True, padding=\"max_length\", return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd0f0ec",
   "metadata": {},
   "source": [
    "## Specter\n",
    "### Tokenize Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04de5b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in tokenizer\n",
    "\n",
    "specter_tokenizer = AutoTokenizer.from_pretrained('allenai/specter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ac98939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "\n",
    "dev_text_tokenized_specter = specter_tokenizer(dev_text, truncation=True, max_length=512, padding=\"max_length\", return_tensors=\"tf\")\n",
    "train_text_tokenized_specter = specter_tokenizer(train_text, truncation=True, max_length=512, padding=\"max_length\", return_tensors=\"tf\")\n",
    "test_text_tokenized_specter = specter_tokenizer(test_text, truncation=True, max_length=512, padding=\"max_length\", return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a386fa",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5f1fe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ensemble_model(dropout=0.3,\n",
    "                            learning_rate=0.00002):\n",
    "    \n",
    "    \"\"\"Build FFNN on top of concatenated hidden states\"\"\"\n",
    "    \n",
    "    longformer_model = TFLongformerModel.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "    specter_model = TFAutoModel.from_pretrained('allenai/specter')\n",
    "    \n",
    "    longformer_model.trainable = True\n",
    "    specter_model.trainable = True\n",
    "    \n",
    "    input_ids_longformer = tf.keras.layers.Input(shape=(4096,), dtype=tf.int32, name=\"longformer_input\") # max_len for Longformer\n",
    "    input_ids_specter = tf.keras.layers.Input(shape=(512,), dtype=tf.int32, name=\"specter_input\") # max_len for Specter\n",
    "    \n",
    "    longformer_out = longformer_model(input_ids_longformer).last_hidden_state\n",
    "    specter_out = specter_model(input_ids_specter).last_hidden_state\n",
    "    \n",
    "    identity = tf.eye(4096, 512)\n",
    "    specter_padded = tf.matmul(identity, specter_out) # Reshape to (None, 4096, 768) for concatenation\n",
    "    concatenated_input = tf.keras.layers.Concatenate(axis=2, name=\"final_concat\")([longformer_out, specter_padded])\n",
    "    \n",
    "    hidden_1 = tf.keras.layers.Dense(512, activation=\"relu\", name=\"hidden_1\")(concatenated_input)\n",
    "    hidden_1 = tf.keras.layers.Dropout(dropout)(hidden_1)\n",
    "    hidden_2 = tf.keras.layers.Dense(256, activation=\"relu\", name=\"hidden_2\")(hidden_1)\n",
    "    hidden_2 = tf.keras.layers.Dropout(dropout)(hidden_2)\n",
    "    \n",
    "    classification = tf.keras.layers.Dense(2, activation=\"sigmoid\", name=\"classification_layer\")(hidden_2)\n",
    "    \n",
    "    classification_model = tf.keras.Model(inputs=[input_ids_longformer, input_ids_specter], outputs=[classification])\n",
    "    \n",
    "    classification_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                                 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), \n",
    "                                 metrics=\"accuracy\")\n",
    "    \n",
    "    return classification_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1181643",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at allenai/longformer-base-4096 were not used when initializing TFLongformerModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFLongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFLongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFLongformerModel were initialized from the model checkpoint at allenai/longformer-base-4096.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFLongformerModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFBertModel.\n",
      "\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at allenai/specter.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = create_ensemble_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff92030b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " specter_input (InputLayer)     [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " longformer_input (InputLayer)  [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109938432   ['specter_input[0][0]']          \n",
      "                                thPoolingAndCrossAt                                               \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 512,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " tf_longformer_model (TFLongfor  TFLongformerBaseMod  148659456  ['longformer_input[0][0]']       \n",
      " merModel)                      elOutputWithPooling                                               \n",
      "                                (last_hidden_state=                                               \n",
      "                                (None, 4096, 768),                                                \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None,                                                \n",
      "                                global_attentions=N                                               \n",
      "                                one)                                                              \n",
      "                                                                                                  \n",
      " tf.linalg.matmul (TFOpLambda)  (None, 4096, 768)    0           ['tf_bert_model[0][0]']          \n",
      "                                                                                                  \n",
      " final_concat (Concatenate)     (None, 4096, 1536)   0           ['tf_longformer_model[0][0]',    \n",
      "                                                                  'tf.linalg.matmul[0][0]']       \n",
      "                                                                                                  \n",
      " hidden_1 (Dense)               (None, 4096, 512)    786944      ['final_concat[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_86 (Dropout)           (None, 4096, 512)    0           ['hidden_1[0][0]']               \n",
      "                                                                                                  \n",
      " hidden_2 (Dense)               (None, 4096, 256)    131328      ['dropout_86[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_87 (Dropout)           (None, 4096, 256)    0           ['hidden_2[0][0]']               \n",
      "                                                                                                  \n",
      " classification_layer (Dense)   (None, 4096, 2)      514         ['dropout_87[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 259,516,674\n",
      "Trainable params: 259,516,674\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "250cd685",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0', 'tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0', 'tf_longformer_model/longformer/pooler/dense/kernel:0', 'tf_longformer_model/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-25 21:25:55.888039: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: model/tf_longformer_model/longformer/encoder/layer_._2/attention/self/cond_2/branch_executed/_277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1014/1014 [==============================] - 14018s 14s/step - loss: 0.0500 - accuracy: 0.9811\n",
      "Epoch 2/2\n",
      "1014/1014 [==============================] - 13857s 14s/step - loss: 0.0357 - accuracy: 0.9861\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([train_text_tokenized[\"input_ids\"], train_text_tokenized_specter[\"input_ids\"]],\n",
    "                    train_labels,\n",
    "                    batch_size=1,\n",
    "                    epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b17d708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "\n",
    "model.save(\"../models/ensemble.h5\", save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "545ba17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [0.050023335963487625, 0.03573835641145706], 'accuracy': [0.9810535907745361, 0.9860784411430359]}\n"
     ]
    }
   ],
   "source": [
    "print(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95843677",
   "metadata": {},
   "source": [
    "# Assess Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b09cea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model([test_text_tokenized[\"input_ids\"], test_text_tokenized_specter[\"input_ids\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a2904a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_token_class_ids = tf.math.argmax(preds, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7963efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(127, 4096), dtype=int64, numpy=\n",
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_token_class_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5efad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predicted_token_class_ids to avoid running inference again\n",
    "\n",
    "np.savetxt(\"../predictions/ensemble_tab_test_set.txt\", predicted_token_class_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec2c34c",
   "metadata": {},
   "source": [
    "## Wikipedia\n",
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fa7ef28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load wiki data\n",
    "\n",
    "with open(\"../data/raw/wiki-summaries/annotated_wikipedia.json\") as file:\n",
    "    wiki_file = json.load(file)\n",
    "\n",
    "with open(\"../data/processed/wiki_masks.json\") as file:\n",
    "    wiki_masks = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e85a627",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 17:02:54.240638: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Create labels\n",
    "\n",
    "wiki_text = []\n",
    "wiki_labels = []\n",
    "\n",
    "for i in range(len(wiki_file)):\n",
    "    doc_id = wiki_file[i][\"doc_id\"]\n",
    "    spans_to_mask = wiki_masks[doc_id]\n",
    "    spans_to_mask = list({tuple(x) for x in spans_to_mask}) # Make spans unique\n",
    "    doc_text = wiki_file[i][\"text\"]\n",
    "    tok_tensor = tokenizer(doc_text, return_tensors=\"tf\", truncation=True, padding=True, return_offsets_mapping=True)\n",
    "    tokens = tok_tensor[\"input_ids\"].numpy()[0]\n",
    "    offsets = tok_tensor[\"offset_mapping\"].numpy()[0]\n",
    "    wiki_text.append(doc_text)\n",
    "    wiki_labels.append(label_tokens(tokens, offsets, spans_to_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1cd096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad labels to max length\n",
    "\n",
    "MAX_LEN = 4096\n",
    "\n",
    "for i in range(len(wiki_labels)):\n",
    "    curr_len = len(wiki_labels[i])\n",
    "    \n",
    "    if curr_len < MAX_LEN:\n",
    "        to_add = [0] * (MAX_LEN - curr_len)\n",
    "        wiki_labels[i].extend(to_add)\n",
    "        \n",
    "wiki_labels = np.asarray(wiki_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c460fe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize input\n",
    "\n",
    "wiki_text_tokenized = tokenizer(wiki_text, truncation=True, padding=\"max_length\", return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a74e872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize input for Specter\n",
    "\n",
    "wiki_text_tokenized_specter = specter_tokenizer(wiki_text, truncation=True, max_length=512, padding=\"max_length\", return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1935f4b",
   "metadata": {},
   "source": [
    "### Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f5f760b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at allenai/longformer-base-4096 were not used when initializing TFLongformerModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFLongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFLongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFLongformerModel were initialized from the model checkpoint at allenai/longformer-base-4096.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFLongformerModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFBertModel.\n",
      "\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at allenai/specter.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load these because they're custom objects\n",
    "\n",
    "longformer_model = TFLongformerModel.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "specter_model = TFAutoModel.from_pretrained('allenai/specter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "196f51bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(\"../models/ensemble.h5\", custom_objects={\"TFBertModel\": specter_model, \"TFLongformerModel\": longformer_model})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cca2c7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 17:22:44.052083: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 104375255040 exceeds 10% of free system memory.\n",
      "2022-11-26 17:23:42.512935: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 104579112960 exceeds 10% of free system memory.\n",
      "2022-11-26 17:23:59.657352: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 104579112960 exceeds 10% of free system memory.\n",
      "2022-11-26 17:25:41.711498: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 83717652480 exceeds 10% of free system memory.\n",
      "2022-11-26 17:25:47.004886: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 83608928256 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "preds = model([wiki_text_tokenized[\"input_ids\"], wiki_text_tokenized_specter[\"input_ids\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb457d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_token_class_ids = tf.math.argmax(preds, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89a2e7d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(553, 4096), dtype=int64, numpy=\n",
       "array([[0, 1, 1, ..., 0, 0, 0],\n",
       "       [0, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]])>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_token_class_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c75a86c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predicted_token_class_ids to avoid running inference again\n",
    "\n",
    "np.savetxt(\"../predictions/ensemble_wiki_preds.txt\", predicted_token_class_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750f92e2",
   "metadata": {},
   "source": [
    "# Calculate Precision, Recall, and AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56fd340a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_precision(pred_list, label_list):\n",
    "    \"\"\"Calculates precision of batch of predictions\"\"\"\n",
    "    \n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    \n",
    "    for i in range(len(pred_list)):\n",
    "        for j in range(len(pred_list[i])):\n",
    "        \n",
    "            if pred_list[i][j] == 1:\n",
    "                if label_list[i][j] == 1:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    fp += 1\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "    return tp / (tp + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e53eae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_recall(pred_list, label_list):\n",
    "    \"\"\"Calculates recall of batch of predictions\"\"\"\n",
    "    \n",
    "    tp = 0 \n",
    "    fn = 0\n",
    "    \n",
    "    for i in range(len(pred_list)):\n",
    "        for j in range(len(pred_list[i])):\n",
    "            \n",
    "            if pred_list[i][j] == 1:\n",
    "                if label_list[i][j] == 1:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    tp += 0\n",
    "                \n",
    "            else:\n",
    "                if label_list[i][j] == 1:\n",
    "                    fn += 1\n",
    "                else:\n",
    "                    fn += 0\n",
    "    \n",
    "    return tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e186f1ee",
   "metadata": {},
   "source": [
    "## Test Set - Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5522c283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Token level precision: 0.955564228367529\n"
     ]
    }
   ],
   "source": [
    "precision = calc_precision(predicted_token_class_ids, test_labels)\n",
    "print (f' Token level precision: {precision}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75ad97ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Token level recall: 0.7242953133584077\n"
     ]
    }
   ],
   "source": [
    "recall = calc_recall(predicted_token_class_ids, test_labels)\n",
    "print (f' Token level recall: {recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3908f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Average AUC: 0.8771953319725475\n"
     ]
    }
   ],
   "source": [
    "# Multilabel indicators are not supported in sklearn for AUC\n",
    "# Loop through preds and take avg of AUC\n",
    "\n",
    "auc = []\n",
    "\n",
    "for i in range(len(predicted_token_class_ids)):\n",
    "    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(test_labels[i], predicted_token_class_ids[i], pos_label=1)\n",
    "    auc.append(metrics.auc(fpr, tpr))\n",
    "\n",
    "auc = sum(auc)/len(auc)\n",
    "print (f' Average AUC: {auc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f152014b",
   "metadata": {},
   "source": [
    "## Wikipedia - Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "931828eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Token level precision: 0.8947179464165508\n"
     ]
    }
   ],
   "source": [
    "precision = calc_precision(predicted_token_class_ids, wiki_labels)\n",
    "print (f' Token level precision: {precision}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d3bb6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Token level recall: 0.8001364530240398\n"
     ]
    }
   ],
   "source": [
    "recall = calc_recall(predicted_token_class_ids, wiki_labels)\n",
    "print (f' Token level recall: {recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae21e993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Average AUC: 0.9140386819371921\n"
     ]
    }
   ],
   "source": [
    "# Multilabel indicators are not supported in sklearn for AUC\n",
    "# Loop through preds and take avg of AUC\n",
    "\n",
    "auc = []\n",
    "\n",
    "for i in range(len(predicted_token_class_ids)):\n",
    "    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(wiki_labels[i], predicted_token_class_ids[i], pos_label=1)\n",
    "    auc.append(metrics.auc(fpr, tpr))\n",
    "\n",
    "auc = sum(auc)/len(auc)\n",
    "print (f' Average AUC: {auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebc80a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
